[
  {
    "question": "Why the Google File System uses large chunk?",
    "options": [
      {
        "text": "To maximize the read and write throughput",
        "image": ""
      },
      {
        "text": "To optimize performance for large files and to reduce the amount of metadata maintained by the system",
        "image": ""
      },
      {
        "text": "To avoid network disconnections during the file access",
        "image": ""
      },
      {
        "text": "To minimize the seek time",
        "image": ""
      },
      {
        "text": "To reduce the time needed to replicate the chunks",
        "image": ""
      },
      {
        "text": "To reduce the number of access to the same chunk",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Large chunks reduce metadata overhead (fewer chunk handles needed) and optimize for large sequential I/O operations common in their workloads, improving overall system efficiency.",
    "hint": "Consider how chunk size affects the amount of metadata the master must store and manage."
  },
  {
    "question": "Which is the key operation performed by the MapReduce framework (like Hadoop) that allow to properly execute the Reduce task(s) after the completion of Map tasks?",
    "options": [
      {
        "text": "Combine",
        "image": ""
      },
      {
        "text": "There is no need of any specific operation",
        "image": ""
      },
      {
        "text": "Shuffle and sort",
        "image": ""
      },
      {
        "text": "Shuffle and Combine",
        "image": ""
      },
      {
        "text": "Combine and sort",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The shuffle phase transfers intermediate key-value pairs from mappers to reducers, while sort organizes these pairs by key so reducers can process them in order.",
    "hint": "What happens to the Map output before it reaches the Reduce function?"
  },
  {
    "question": "What is the key idea behind MapReduce?",
    "options": [
      {
        "text": "Master - Slave algorithm design paradigm",
        "image": ""
      },
      {
        "text": "Single Data Multiple Processing algorithm design paradigm",
        "image": ""
      },
      {
        "text": "Batch parallel processing",
        "image": ""
      },
      {
        "text": "Data Stream processing",
        "image": ""
      },
      {
        "text": "Divide and conquer algorithm design paradigm",
        "image": ""
      }
    ],
    "correctIndex": 4,
    "image": "",
    "code": "",
    "explanation": "MapReduce splits large datasets into smaller chunks (divide), processes them in parallel with Map functions (conquer), then combines results with Reduce functions.",
    "hint": "How does the framework break down and process large computational tasks?"
  },
  {
    "question": "Assume you setup a multi-threshold scale-out policy with two thresholds on the CPU utilization (Ucpu), namely Tcpu1=50% of max CPU utilization and Tcpu2=80% of max CPU utilization. A scaling action start if (see the figure): \n\n( Ucpu > Tcpu1 ) OR ( Ucpu > Tcpu2 ) AND (the threshold is exceeded for more than 300 seconds). \n\nWhy is important to wait a certain time interval before starting a scaling action?",
    "options": [
      {
        "text": "Because 300 secs is the time required by a new instance to start and to join the scaling group",
        "image": ""
      },
      {
        "text": "To limit the number of alarms generated in a given time interval, e.g. one hours.",
        "image": ""
      },
      {
        "text": "To avoid ping-pong effects that make the scaling policy unstable",
        "image": ""
      },
      {
        "text": "To guarantee that the added resources are warm-up",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The waiting period prevents reacting to transient CPU spikes that would cause the system to rapidly add and remove instances (ping-pong effect), making the scaling policy unstable.",
    "hint": "What would happen if the system immediately scaled up/down at every brief CPU fluctuation?"
  },
  {
    "question": "Which of the following are essential characteristics of cloud computing? (as defined by NIST)",
    "options": [
      {
        "text": "Broad network access; Resource pooling; virtualization",
        "image": ""
      },
      {
        "text": "On-demand self-service; Broad network access; and rapid elasticity ",
        "image": ""
      },
      {
        "text": "Resource pooling; Measured Services; and Scalability",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "NIST defines three essential cloud characteristics: on-demand self-service (resources without human interaction), broad network access (ubiquitous access), and rapid elasticity (automatic scaling).",
    "hint": "Which three characteristics did NIST officially designate as essential for cloud computing?"
  },
  {
    "question": "Which of the following are essential characteristics of cloud computing? (as defined by NIST)",
    "options": [
      {
        "text": "Resource pooling; Measured Services; and Scalability",
        "image": ""
      },
      {
        "text": "On-demand self-service; scalability; and monitoring",
        "image": ""
      },
      {
        "text": "Resource pooling; rapid elasticity; and measured services",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Le tre caratteristiche essenziali definite da NIST sono Resource pooling (condivisione delle risorse), Rapid elasticity (elasticità rapida) e Measured services (servizi misurati). Le altre opzioni includono caratteristiche non belonging alla definizione NIST o termini meno precisi.",
    "hint": "Ricorda le 5 caratteristiche NIST: on-demand, broad network access, resource pooling, rapid elasticity, measured service."
  },
  {
    "question": "Which, among the following definitions describe the MAPE-K cycle",
    "options": [
      {
        "text": "The control loop for scaling, load balancing and VMs migration.",
        "image": ""
      },
      {
        "text": "The control loop implemented by an autonomic manager",
        "image": ""
      },
      {
        "text": "The control loop executed by a cloud monitor",
        "image": ""
      },
      {
        "text": "The control loop executed to build the Knowledge about the system state",
        "image": ""
      },
      {
        "text": "The monitoring, analytics, placement and execution control loop",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "MAPE-K è l'acronimo di Monitor-Analyze-Plan-Execute over Knowledge, ed è il ciclo di controllo fondamentale del computing autonomico implementato da un autonomic manager che permette di gestire sistemi self-managing.",
    "hint": "MAPE-K è il ciclo di controllo tipico dei sistemi autonomici/auto-gestiti."
  },
  {
    "question": "Suppose that you would like to deploy a web application composed by: a frontend server to receive http requests and produce http reply/; a business logic server to build the content of the web pages; a database server, used by the business logic to extract information. Suppose also that the business logic component should scale (horizontally). Which system architectural style/model could be used for the deployment?",
    "options": [
      {
        "text": "Microservice architecture model",
        "image": ""
      },
      {
        "text": "Multi-tier client-server architecture model",
        "image": ""
      },
      {
        "text": "Service oriented architecture model",
        "image": ""
      },
      {
        "text": "IaaS architecture model",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "L'architettura descritta presenta tre livelli distinti (frontend, business logic, database) che comunicano in modo sequenziale, caratteristica tipica del modello multi-tier client-server. I microservices richiederebbero che ogni componente fosse un servizio indipendente con API propria.",
    "hint": "Distigui tra tier (livelli funzionali) e microservice (servizi indipendenti)."
  },
  {
    "question": "Which type of scalability is facilitated by microservices",
    "options": [
      {
        "text": "Scaling of single functions",
        "image": ""
      },
      {
        "text": "Application cloning",
        "image": ""
      },
      {
        "text": "Scaling of function's data",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "I microservices permettono di scalare orizzontalmente singole funzioni o componenti in modo indipendente dagli altri, cosa non possibile in un'architettura monolitica dove si scala l'intera applicazione.",
    "hint": "I microservices scompongono l'applicazione in parti indipendenti scalabili."
  },
  {
    "question": "Which are the business drivers for cloud computing?",
    "options": [
      {
        "text": "Capacity Planning, Increase of business revenue, Agility",
        "image": ""
      },
      {
        "text": "Capacity Planning, Cost Reduction, Business Agility",
        "image": ""
      },
      {
        "text": "DevOp, Capacity Planning, Increased Security",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "I principali business driver per il cloud computing sono: Capacity Planning (pianificazione ottimale delle risorse), Cost Reduction (riduzione dei costi infrastrutturali) e Business Agility (maggiore velocità di risposta al mercato).",
    "hint": "I driver includono benefici economici e operativi per il business."
  },
  {
    "question": "Assume that you are setting up a scalable web server with AWS and that for your autoscaling group you have configure a step scaling policy. You have decided to set the alarm to start the scale-out with a threshold on the incoming traffic. Which of the following assertions is true?",
    "options": [
      {
        "text": "The alarm occurs if for the majority of the instances in the scaling group the value of the incoming traffic exceeds the threshold (e.g. for 3 out of 5).",
        "image": ""
      },
      {
        "text": "The alarm occurs if the average value of the incoming traffic computed over all the instances in the scaling group exceed the threshold.",
        "image": ""
      },
      {
        "text": "The alarm occurs if there is at least an instance in the scaling group with the value of the incoming traffic exceeding the threshold.",
        "image": ""
      },
      {
        "text": "The alarm occurs if for a user configured number of the instances in the scaling group the value of the incoming traffic exceeds the threshold.",
        "image": ""
      },
      {
        "text": "The alarm occurs if for all the instances in the scaling group the value of the incoming traffic exceeds the threshold.",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Nelle policy di step scaling AWS, le metriche come il traffico in ingresso vengono calcolate come media aggregata su tutte le istanze del gruppo Auto Scaling, non valutate singolarmente per ciascuna istanza.",
    "hint": "Ricorda che il dimensionamento automatico considera le metriche aggregate dell'intero gruppo, non le singole istanze."
  },
  {
    "question": "In what differ microservices and the SOA?",
    "options": [
      {
        "text": "Communication mechanisms, Data handling, and Size of the services",
        "image": ""
      },
      {
        "text": "Communication protocols, data storage solutions, complexity of the service",
        "image": ""
      },
      {
        "text": "Communication mechanisms, Data partitioning, Coupling of the services",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "I microservizi rappresentano un'evoluzione di SOA con comunicazione più leggera (REST), gestione dati autonoma per ogni servizio (database dedicato), e servizi di dimensioni più ridotte e indipendenti.",
    "hint": "I microservizi sono una forma più granulare e specifica di architettura orientata ai servizi."
  },
  {
    "question": "Assume that you are deploying a set of containers and that the network stack of the containers should not be isolated from the Docker host. Which of the following network drivers will you use?",
    "options": [
      {
        "text": "Macvlan networks",
        "image": ""
      },
      {
        "text": "User-defined bridge networks",
        "image": ""
      },
      {
        "text": "Host Networks",
        "image": ""
      },
      {
        "text": "Overlay networks",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Il driver host in Docker rimuove completamente l'isolamento della rete tra il container e l'host, facendo sì che il container condivida direttamente lo stack di rete dell'host Docker.",
    "hint": "Per non avere isolamento dalla rete dell'host, bisogna usare il driver che condivide lo stack di rete."
  },
  {
    "question": "What is the difference between hypervisor of type I and hypervisor of type II?",
    "options": [
      {
        "text": "Type I hypervisors run directly on top of the hardware of the host server and Type II hypervisors run on top of the operating system of the host server.",
        "image": ""
      },
      {
        "text": "Type I hypervisors use hardware assisted virtualization and type II hypervisors use software assisted virtualization",
        "image": ""
      },
      {
        "text": "Type I hypervisors use para-virtualization and type II hypervisors use full virtualization",
        "image": ""
      },
      {
        "text": "Type I hypervisors use binary translation and type II hypervisors use hypercalls",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Gli hypervisor Type I (bare-metal) vengono installati direttamente sull'hardware senza bisogno di un sistema operativo host, mentre gli hypervisor Type II (hosted) richiedono un sistema operativo ospite su cui girare.",
    "hint": "La distinzione principale è dove viene eseguito l'hypervisor: direttamente sull'hardware o sopra un sistema operativo."
  },
  {
    "question": "Consider the live migration process and in the specific the “iterative pre-copy” step. Why the memory is copied iteratively?",
    "options": [
      {
        "text": "To preserve continuity in the migration process",
        "image": ""
      },
      {
        "text": "To make, step-by-step, the dirty portion of the memory, on the VM to be migrated, small enough to handle the final copy ",
        "image": ""
      },
      {
        "text": "Because usually the memory of a VMs is too large to be transferred in one shot.",
        "image": ""
      },
      {
        "text": "Because to transfer the memory of a VM in one shout consume too much bandwidth",
        "image": ""
      },
      {
        "text": "To avoid overload on the virtual machine to be migrated",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "La copia iterativa della memoria riduce progressivamente la porzione di memoria 'dirty' (modificata) ad ogni ciclo, così che l'ultimo trasferimento (downtime) sia最小 e gestibile.",
    "hint": "L'obiettivo è rendere la quantità di memoria rimanente da copiare sufficientemente piccola per minimizzare il tempo di inattività."
  },
  {
    "question": "Why system level virtualization (a.k.a. Hypervisor) increase security?",
    "options": [
      {
        "text": "Because the hypervisor run in user mode ",
        "image": ""
      },
      {
        "text": "Because the hypervisor control and filters the execution of privileged instructions invoked by the guest operating system",
        "image": ""
      },
      {
        "text": "Because the guest operating systems cannot access the physical resources and then cannot do harmful operations",
        "image": ""
      },
      {
        "text": "Because the hypervisor run in privileged mode",
        "image": ""
      },
      {
        "text": "Because the guest operating system use hyper-call and hardware assisted virtualization",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "The hypervisor acts as a control layer between guest OSes and hardware, intercepting and validating privileged instructions (like I/O operations) before they reach physical resources, preventing direct hardware access that could be exploited.",
    "hint": "Consider what happens when a guest OS attempts privileged operations - who controls this access?"
  },
  {
    "question": "Consider the Kubernetes architecture in the figure. Assume you configure the container autoscaling mechanisms. What is the missing component in the figure?",
    "options": [
      {
        "text": "A resource usage monitor module",
        "image": ""
      },
      {
        "text": "The pod module",
        "image": ""
      },
      {
        "text": "The planner module",
        "image": ""
      },
      {
        "text": "The analyzer module",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Autoscaling mechanisms require continuous monitoring of resource consumption (CPU, memory) to determine when scaling triggers thresholds are met, making a monitoring module essential.",
    "hint": "Autoscaling needs to measure current resource utilization to make scaling decisions."
  },
  {
    "question": "Consider the Kubernetes architecture in the figure. What is the component that takes scaling decisions?",
    "options": [
      {
        "text": "The kube-apiserver",
        "image": ""
      },
      {
        "text": "The kube-scheduler",
        "image": ""
      },
      {
        "text": "The kube-controller-manager",
        "image": ""
      },
      {
        "text": "The cloud-controller manager",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The kube-controller-manager runs controller loops that continuously monitor cluster state and take corrective actions, including scaling decisions for pods based on desired state.",
    "hint": "Which Kubernetes component runs the reconciliation loops for maintaining desired state?"
  },
  {
    "question": "Consider the Kubernetes architecture in the figure. What is the component that decides on which Kubernetes node to start a new container?",
    "options": [
      {
        "text": "The kube-scheduler ",
        "image": ""
      },
      {
        "text": "The cloud-controller manager",
        "image": ""
      },
      {
        "text": "The kube-controller manager",
        "image": ""
      },
      {
        "text": "The kube-apiserver",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The kube-scheduler evaluates pod requirements against node resources and policies (affinity, taints, capacity) to determine the optimal node for placing new containers.",
    "hint": "There's a specific component responsible for pod-to-node assignment based on constraints."
  },
  {
    "question": "Consider the MAPE-K cycle in the figure. What are the AWS components that implement the Monitor, Analysis and Plan phases?",
    "options": [
      {
        "text": "Monitor=CloudWatch, Analysis=Alarms, Plan=Dynamic Scaling",
        "image": ""
      },
      {
        "text": "Monitor=CloudWatch, Analysis=Metrics, Plan=Dynamic Scaling ",
        "image": ""
      },
      {
        "text": "Monitor=CloudWatch, Analysis=CloudWatch, Plan=Elastic Load Balancer",
        "image": ""
      },
      {
        "text": "Monitor=Metrics, Analysis=Thresholds, Plan=Scale-in/out policy",
        "image": ""
      },
      {
        "text": "Monitor=Metrics, Analysis=Alarms, Plan=Thresholds",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "CloudWatch monitors AWS resources and generates metrics, Alarms analyze these metrics against thresholds to trigger scaling, and Dynamic Scaling policies execute the scaling plan.",
    "hint": "Think about which AWS services correspond to observing, evaluating, and acting on metrics."
  },
  {
    "question": "Consider the BigTable datastore architecture in the figure. Which among the followings are duties of the Tablet server?",
    "options": [
      {
        "text": "To split tablets that have grown too large; to assign tablets to tablet servers; to detect the addition and expiration of tablet servers",
        "image": ""
      },
      {
        "text": "To manage a set of tablets; to handle read and write requests to the tablets that it has loaded; to split tablets that have grown too large",
        "image": ""
      },
      {
        "text": "To balance tablet-server load, and garbage collection of files in GFS; to handle schema changes such as table and column family creations; to handle read and write requests",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Il tablet server gestisce le operazioni sui dati: gestisce un insieme di tablet, gestisce le richieste di lettura/scrittura e divide tablet troppo grandi. Il Master si occupa invece dell'assegnazione e del bilanciamento del carico.",
    "hint": "Ricorda la distinzione tra gestione dei dati (tablet server) e coordinamento (Master)."
  },
  {
    "question": "Consider the architecture of the HDFS in the figure. Which among the following actions are performed by the NameNode?",
    "options": [
      {
        "text": "To manages the File System Namespace; to controls access to files; to store metadata",
        "image": ""
      },
      {
        "text": "To manages the File System Namespace; to write in the journal log; to handle the communication with the data nodes",
        "image": ""
      },
      {
        "text": "To record changes in a log; to check DataNode liveness; to write in a file",
        "image": ""
      },
      {
        "text": "To write in a file; to manage the cryptographic keys; to handle the communication with the data nodes",
        "image": ""
      },
      {
        "text": "To write in a file; to write in the journal log; to restart/replace failing DataNode",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Il NameNode mantiene i metadati del filesystem distribuito, gestisce lo spazio dei nomi e controlla gli accessi ai file. Non gestisce direttamente la comunicazione con i DataNode per il trasferimento dati.",
    "hint": "Il NameNode è il coordinatore centrale che gestisce metadati e permessi, non il trasferimento dati."
  },
  {
    "question": "Consider the Google File System architecture in the figure. What components (or component) are responsible to handle the file access?",
    "options": [
      {
        "text": "Application and chunk server",
        "image": ""
      },
      {
        "text": "Application, master and chunk server",
        "image": ""
      },
      {
        "text": "Master and chunk server",
        "image": ""
      },
      {
        "text": "The linux file system",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "In GFS, i client comunicano direttamente con i chunk server per leggere/scrivere i dati. Il master interviene solo per le operazioni sui metadati, non per l'accesso effettivo ai file.",
    "hint": "GFS evita colli di bottiglia facendo comunicare client e chunk server direttamente per i dati."
  },
  {
    "question": "Researchers and engineers at Google designed the Google File System taking into account the file access model. What is the most frequent type of file access?",
    "options": [
      {
        "text": "Random write",
        "image": ""
      },
      {
        "text": "Random read",
        "image": ""
      },
      {
        "text": "Sequential read",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "GFS è ottimizzato per le letture sequenziali perché i job MapReduce tipici leggono grandi file in modo sequenziale. Questo ha influenzato molte scelte progettuali di GFS.",
    "hint": "Pensa ai tipici carichi di lavoro di elaborazione dati su larga scala."
  },
  {
    "question": "Researchers and engineers at Google designed the Google File System taking into account the file access model. What is the most frequent type of file access?",
    "options": [
      {
        "text": "Random write",
        "image": ""
      },
      {
        "text": "Random read",
        "image": ""
      },
      {
        "text": "Append write",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In GFS l'append è l'operazione di scrittura più frequente perché permette a più writer di scrivere contemporaneamente senza conflitti. GFS è progettato per ottimizzare questa operazione.",
    "hint": "L'append è l'operazione di scrittura dominante in GFS, non la scrittura casuale."
  },
  {
    "question": "What among the following is an RDD transformation?",
    "options": [
      {
        "text": "collect",
        "image": ""
      },
      {
        "text": "reduce",
        "image": ""
      },
      {
        "text": "reduceByKey",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "reduceByKey is a transformation because it returns a new RDD by aggregating values with the same key. In Spark, transformations return new RDDs, while actions return values to the driver program.",
    "hint": "Transformations create new RDDs, actions return values to the driver."
  },
  {
    "question": "What among the following is an RDD transformation?",
    "options": [
      {
        "text": "collect",
        "image": ""
      },
      {
        "text": "reduce",
        "image": ""
      },
      {
        "text": "map",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "map is a transformation that applies a function to each element of the RDD and returns a new RDD. Both collect and reduce are actions that return values to the driver.",
    "hint": "Transformations are lazy operations that produce new RDDs."
  },
  {
    "question": "What among the following is an RDD action?",
    "options": [
      {
        "text": "collect",
        "image": ""
      },
      {
        "text": "map",
        "image": ""
      },
      {
        "text": "reduceByKey",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "collect is an action that returns all elements of the RDD to the driver program. It triggers the computation and brings results back to the driver.",
    "hint": "Actions trigger execution and return results to the driver."
  },
  {
    "question": "Hadoop MapReduce optimizes data locality when map tasks are deployed on the Hadoop cluster, that is Hadoop MapReduce try first to run Map tasks on a data local node to not use cluster bandwidth. In case local optimization is not possible, what alternative deployment policies are used?",
    "options": [
      {
        "text": "Rack local deployment",
        "image": ""
      },
      {
        "text": "Cluster local deployment",
        "image": ""
      },
      {
        "text": "Round-robin deployment",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Hadoop follows a hierarchy of data locality: first tries node-local (same node), then rack-local (same rack), and finally falls back to any available node in the cluster.",
    "hint": "Consider the network topology hierarchy in Hadoop: node, rack, cluster."
  },
  {
    "question": "A MapReduce computation is usually executed on a large cluster of commodity nodes. Hence, the failure probability of the master and worker nodes is very high. In case the master node fails, what action should be taken to preserve the result of the computation?",
    "options": [
      {
        "text": "All the the reduce tasks need to be re-scheduled and executed also if some completed",
        "image": ""
      },
      {
        "text": "All the the map tasks need to be re-scheduled and executed also if some completed",
        "image": ""
      },
      {
        "text": "The entire MapReduce computation must be restarted",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The master node (JobTracker) maintains all job metadata and coordinates the entire computation. If it fails, there is no way to recover the job state, so the entire MapReduce computation must be restarted.",
    "hint": "Without the master node, there is no central coordination to resume or complete the job."
  },
  {
    "question": "A MapReduce computation is usually executed on a large cluster of commodity nodes. Hence, the failure probability of the master and worker nodes is very high. In case a map worker node fail, what action should be taken to preserve the result of the computation?",
    "options": [
      {
        "text": "All the the reduce tasks need to be re-scheduled and executed also if some completed",
        "image": ""
      },
      {
        "text": "All the map tasks assigned to the worker will be re-scheduled and re-executed, also if some completed",
        "image": ""
      },
      {
        "text": "The entire MapReduce computation must be restarted",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In MapReduce, i map task producono risultati intermedi memorizzati localmente sul worker. Se un worker fallisce, i dati intermedi non sono più accessibili, quindi tutti i map task assegnati a quel worker devono essere ri-eseguiti. I reduce task non dipendono dal worker specifico ma solo dai risultati intermedi completati.",
    "hint": "I risultati dei map task sono memorizzati locally sul worker che li esegue, non sul master."
  },
  {
    "question": "Which of the following is a cloud deployment model?",
    "options": [
      {
        "text": "Cloud Service Broker",
        "image": ""
      },
      {
        "text": "Public cloud",
        "image": ""
      },
      {
        "text": "Software as a Service",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "I deployment model definiscono la configurazione dell'infrastruttura cloud (dove è ospitata e chi la gestisce). Public cloud è un deployment model classico, mentre Cloud Service Broker è un pattern architetturale e SaaS è un service model.",
    "hint": "I deployment model riguardano la localizzazione e la proprietà dell'infrastruttura."
  },
  {
    "question": "Which of the following is a cloud service model",
    "options": [
      {
        "text": "Private Cloud",
        "image": ""
      },
      {
        "text": "Docker as a Service",
        "image": ""
      },
      {
        "text": "Infrastructure as a Service",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "I cloud service model (IaaS, PaaS, SaaS) definiscono cosa il provider gestisce. IaaS fornisce infrastruttura virtualizzata. Private Cloud è un deployment model, Docker as a Service non è un termine standard riconosciuto.",
    "hint": "I service model definiscono il livello di astrazione e responsabilità tra provider e utente."
  },
  {
    "question": "Consider the difference between SOA and microservices and specifically the communication mechanisms. What of the following assertion is correct?",
    "options": [
      {
        "text": "Both microservices and SOA use a message broker. SOA message broker is a proprietary technology, while the micrroservices message broker is open source",
        "image": ""
      },
      {
        "text": "Microservices uses lightweight and open-source communication mechanisms, while SOA uses heavyweight and often proprietary communication mechanisms.",
        "image": ""
      },
      {
        "text": "SOA uses lightweight communication mechanisms and microservices uses open source communication mechanisms",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Microservices favoriscono comunicazioni leggere come REST, HTTP e protocolli aperti, mentre l'architettura SOA tradizionale tipicamente usa ESB (Enterprise Service Bus) che sono spesso proprietari e più complessi.",
    "hint": "La differenza principale è nel grado di accoppiamento e nel peso dei middleware di comunicazione."
  },
  {
    "question": "What are the enabling technologies for containers?",
    "options": [
      {
        "text": "Namespaces, cgroups, unionfs",
        "image": ""
      },
      {
        "text": "Docker, Kubernetes, Linux",
        "image": ""
      },
      {
        "text": "Namespace, access control groups, file union mechanisms",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Namespaces forniscono isolamento delle risorse di sistema, cgroups gestiscono l'allocazione e limitazione delle risorse per gruppi di processi, e unionfs permette di unire filesystem a strati (layer). Insieme formano le fondamenta tecniche dei container.",
    "hint": "Sono le funzionalità del kernel Linux che permettono l'isolamento e la virtualizzazione a livello di SO."
  },
  {
    "question": "Assume that you are deploying a set of containers on different Docker hosts, and such containers need to communicate each other. Which network drivers will you use?",
    "options": [
      {
        "text": "User-defined bridge networks",
        "image": ""
      },
      {
        "text": "Overlay Networks",
        "image": ""
      },
      {
        "text": "Macvlan networks",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Overlay networks create a virtual network layer that spans multiple Docker hosts, enabling containers on different hosts to communicate seamlessly as if on the same network, which is essential for distributed container deployments.",
    "hint": "Consider which network driver is specifically designed for multi-host container communication."
  },
  {
    "question": "Considering the write protocol of the HDFS. Who is in charge of sending the “write successful” acknowledgement to the NameNode?",
    "options": [
      {
        "text": "The primary DataNode and all the secondary DataNode involved in the write operation",
        "image": ""
      },
      {
        "text": "The client",
        "image": ""
      },
      {
        "text": "The primary DataNode involved in the write operation",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In HDFS, the client coordinates the write operation, receives acknowledgments from DataNodes, and then notifies the NameNode of successful completion.",
    "hint": "Think about the role of the client in coordinating the write pipeline."
  },
  {
    "question": "A Bigtable is a sparse, distributed, persistent multidimensional sorted map. Which data could be accessed with the key (Row key, column key, timestamp)?",
    "options": [
      {
        "text": "A specific version of a big table cell",
        "image": ""
      },
      {
        "text": "A family cell",
        "image": ""
      },
      {
        "text": "All the cell with the same Row and Column key stored from the time=0 (creation of the table) to time=timestemp",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The combination of row key, column key, and timestamp uniquely identifies a single cell value at a specific point in time, allowing access to particular versions of that cell.",
    "hint": "Remember that timestamps in Bigtable serve a versioning purpose."
  },
  {
    "question": "What is a Cloud Broker?",
    "options": [
      {
        "text": "An entity that manages the use, performance and delivery of cloud services, and negotiates relationships between Cloud Providers and Cloud Consumers.",
        "image": ""
      },
      {
        "text": "An intermediary that provides connectivity and transport of cloud services from Cloud Providers to Cloud Consumers.",
        "image": ""
      },
      {
        "text": "A party that can conduct independent assessment of cloud services, information system operations, performance and security of the cloud implementation.",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "A cloud broker acts as an intermediary that manages, controls, and coordinates cloud service usage between providers and consumers, negotiating relationships and ensuring service delivery.",
    "hint": "Think about the intermediary role in service management."
  },
  {
    "question": "What is Rapid Elasticity?",
    "options": [
      {
        "text": "Is the degree to which a system is able to add or remove virtual machines automatically, without any human in the loop",
        "image": ""
      },
      {
        "text": "Is the degree of workload changes and performance trend changes by provisioning and deprovisioning resources in an automatic manner",
        "image": ""
      },
      {
        "text": "Is the degree to which a system is able to adapt to workload changes by provisioning and deprovisioning resources in an automatic manner",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Rapid elasticity is the cloud characteristic that enables automatic scaling of resources based on workload changes, allowing the system to adapt dynamically to demand fluctuations.",
    "hint": "Focus on the automatic scaling aspect in response to workload changes."
  },
  {
    "question": "What is a namespace?",
    "options": [
      {
        "text": "A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource.",
        "image": ""
      },
      {
        "text": "A namespace is an abstraction that wraps containers running tightly coupled services. Namespaces are implemented as Pod in Kubernetes.",
        "image": ""
      },
      {
        "text": "A namespace is a Linux kernel feature which allow processes to be organized into groups whose usage of various types of resources can then be limited and monitored",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Namespaces are a Linux kernel feature that provide isolation by wrapping global system resources (like PIDs, network interfaces, mount points) so that processes within a namespace see only their own isolated instance of that resource.",
    "hint": "Think about what isolation means - it's about making processes believe they have exclusive access to system resources."
  },
  {
    "question": "Consider the docker CLI command `docker run -i -t ubuntu /bin/bash`, which of the following actions is performed by the command?",
    "options": [
      {
        "text": "Locate and eventually download the ubuntu image, if not locally stored",
        "image": ""
      },
      {
        "text": "Create two containers: the first one running Ubuntu OS and the second one running the bash application",
        "image": ""
      },
      {
        "text": "Run a temporary filesystem and the bash CLI on top of it.",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "When running `docker run ubuntu`, Docker first checks if the ubuntu image exists locally. If not found, it automatically pulls (downloads) the image from a registry like Docker Hub before creating and starting the container.",
    "hint": "Remember the order: Docker needs the image first before it can run a container from it."
  },
  {
    "question": "Consider the Google File System architecture. What is/are the component/s responsible to create a file?",
    "options": [
      {
        "text": "Application and chunk server",
        "image": ""
      },
      {
        "text": "Master and chunk server",
        "image": ""
      },
      {
        "text": "Chunk Server",
        "image": ""
      },
      {
        "text": "The master",
        "image": ""
      },
      {
        "text": "The linux file system",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "In GFS architecture, the master handles all metadata operations including file creation. Chunk servers only store and retrieve actual data chunks based on instructions from the master.",
    "hint": "In GFS, the master manages metadata while chunk servers handle the actual data storage."
  },
  {
    "question": "Which of the following definition of capacity planning is correct?",
    "options": [
      {
        "text": "Capacity planning is a methodology that allows to determine the right amount of capacity when needed to avoid Over-provisioning and Under-provisioning of resources.",
        "image": ""
      },
      {
        "text": "Capacity planning is part of the elasticity essential characteristic of cloud computing enabling on demand self service.",
        "image": ""
      },
      {
        "text": "Capacity planning is a set of technologies enabling cloud computing scalability to avoid over provisioning and under provisioning.",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Capacity planning is fundamentally about determining the optimal amount of resources needed to meet demand, avoiding waste from over-provisioning and performance issues from under-provisioning.",
    "hint": "Focus on the balance between too much and too little resources."
  },
  {
    "question": "Which of the following are benefits of the microservice architecture?",
    "options": [
      {
        "text": "Simple to develop and easy to make radical changes of the whole application; Straightforward to test and deploy; Easy to scale",
        "image": ""
      },
      {
        "text": "Better fault isolation; Cost reduction; Open source technology; Lightweight execution environment",
        "image": ""
      },
      {
        "text": "Maintainability of the services; Independent deployability of services; Independent scalability of services",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Microservices offer maintainability (ease of changing individual services), independent deployability (each service deploys separately), and independent scalability (each service scales based on its own load).",
    "hint": "Think about what can be done independently for each service in a distributed system."
  },
  {
    "question": "Which of the following are challenges in the microservice architecture?",
    "options": [
      {
        "text": "Experimenting and adoption of new technologies; fault isolation;",
        "image": ""
      },
      {
        "text": "Efficient resource management; orchestration; choreography",
        "image": ""
      },
      {
        "text": "Finding the right set of services; to develop, test and deploy complex applications",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The main challenges in microservices are decomposition (finding the right service boundaries) and the operational complexity of developing, testing, and deploying multiple independent services. This differs from the benefits, which include fault isolation and technology flexibility.",
    "hint": "Think about what makes microservices difficult to implement rather than what advantages they provide."
  },
  {
    "question": "What is a microservice architecture?",
    "options": [
      {
        "text": "Is an architectural style that structures an application as a collection of services that are: Highly maintainable and testable;Loosely coupled; Independently deployable; Organized around business capabilities; Owned by a small team.",
        "image": ""
      },
      {
        "text": "Is an architectural style that structures an application as a collection of components having their own life cycles and interacting each other by means of communicating processes or event systems.",
        "image": ""
      },
      {
        "text": "Is an new architectural style merging the good of loosely coupled and Independently deployable event based systems in client server architecture",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "This is the standard definition from practitioners like Martin Fowler and Adrian Cockcroft, emphasizing the key characteristics: maintainability, loose coupling, independent deployment, business capability alignment, and small team ownership.",
    "hint": "Look for the definition that includes all the canonical microservice characteristics from the literature."
  },
  {
    "question": "A Bigtable is a sparse, distributed, persistent multidimensional sorted map. Which of the following keys does not allow to locate a cell?",
    "options": [
      {
        "text": "Row key, column key and timestamp",
        "image": ""
      },
      {
        "text": "Row key, family key and timestamp",
        "image": ""
      },
      {
        "text": "Row key and column key",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Bigtable's data model uses (row key, column key, timestamp) to locate a cell. Column families are logical groupings of columns, not part of the cell lookup key - the column key itself contains the family qualifier.",
    "hint": "Recall that Bigtable organizes data with row, column (which includes family), and timestamp as the three dimensions."
  },
  {
    "question": "Bigtable uses timestamp to maintain versioning of the data stored. What among the followings is a mechanism to garbage collect cell versions automatically?",
    "options": [
      {
        "text": "Only new-enough versions be kept",
        "image": ""
      },
      {
        "text": "The last n versions that are new-enough be kept",
        "image": ""
      },
      {
        "text": "The last n versions modified in the last 7 days be kept",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Bigtable's garbage collection is time-based: it keeps only versions that are 'new-enough' (recent enough based on a configurable age). This is the automatic mechanism to prune old versions.",
    "hint": "Think about what 'new-enough' means in the context of time-based versioning."
  },
  {
    "question": "The NIST definition of Cloud computing mention that “Computing resources are pooled to serve multiple consumers using a multi-tenant model”. What does multi-tenancy means?",
    "options": [
      {
        "text": "Multiple users (tenants) access the same application logic simultaneously",
        "image": ""
      },
      {
        "text": "Multiple users (tenants) access the same virtual host simultaneously",
        "image": ""
      },
      {
        "text": "Multiple users (tenants) access the same data and configurations simultaneously",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Multi-tenancy means multiple tenants share the same application instance and logic, but each tenant's data is isolated. This is a fundamental cloud computing concept for resource pooling and efficiency.",
    "hint": "Focus on what is shared (application logic) versus what is isolated (data) in multi-tenant systems."
  },
  {
    "question": "What is a capacity planning match strategy?",
    "options": [
      {
        "text": "A strategy for adding capacity when the IT resource reaches its full capacity",
        "image": ""
      },
      {
        "text": "A strategy for adding capacity to an IT resource in anticipation of demand",
        "image": ""
      },
      {
        "text": "A strategy for adding IT resource capacity in small increments, as demand increases",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The capacity planning 'match' strategy specifically adds capacity in small incremental steps to mirror demand increases, unlike 'lead' (anticipatory) or 'lag' (reactive) strategies.",
    "hint": "Think about what 'matching' something means - it should follow or mirror the pattern incrementally."
  },
  {
    "question": "Assume you are using a service offered via a web application and you do not pay for the use of the service. Which feature the service should have to be classified as a cloud service?",
    "options": [
      {
        "text": "The service should provide the illusion of having infinite resources",
        "image": ""
      },
      {
        "text": "The service should be capable to scale in or out",
        "image": ""
      },
      {
        "text": "The service should allow  to control security and privacy",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The illusion of infinite resources is a fundamental cloud characteristic that enables elasticity and on-demand self-service, abstracting resource limitations from the user.",
    "hint": "What makes cloud feel different from traditional infrastructure regarding resource availability?"
  },
  {
    "question": "The Ansible automation tools is defined to be an “agentless tool that runs in a push model”. What a push model means?",
    "options": [
      {
        "text": "It is required to install specific software component on the remote machines to make them receive instruction pushed by the Ansible server",
        "image": ""
      },
      {
        "text": "It manages remote machines by passing instructions (Ansible modules) to remote management framework already existing on the platforms to be administered",
        "image": ""
      },
      {
        "text": "The management tool does not create any overhead when administration tasks are not running",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Agentless push model means Ansible connects directly to managed hosts via existing frameworks (like SSH) and pushes modules without requiring permanent agent software on remote machines.",
    "hint": "Combine the meaning of 'agentless' with 'push' - how does it communicate without agents?"
  },
  {
    "question": "Suppose you have two implementation and deployment of the same application, one based on the Service Oriented Architecture and the other based on the Microservice Architecture. What will be the main difference between the two implementation and deployment?",
    "options": [
      {
        "text": "Scaling algorithms used; reusability of the services; level of coupling of the services",
        "image": ""
      },
      {
        "text": "Scaling algorithms used; data storage technologies; level of coupling of the services",
        "image": ""
      },
      {
        "text": "Communication mechanisms used; Data handling model adopted; and size of the services",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Microservices emphasize small, independently deployable services with their own data stores using lightweight communication (REST), while SOA uses larger coarse-grained services with shared databases and heavier communication protocols.",
    "hint": "Consider the fundamental architectural differences in how services communicate and manage data."
  },
  {
    "question": "An essential characteristic of cloud computing is measured service. How AWS implement this feature?",
    "options": [
      {
        "text": "With CloudWatch",
        "image": ""
      },
      {
        "text": "With EBS",
        "image": ""
      },
      {
        "text": "With autoscaling groups",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "CloudWatch provides monitoring, metrics collection, and metering for AWS resources, enabling the pay-per-use model that characterizes measured service in cloud computing.",
    "hint": "Which AWS service is specifically designed for monitoring resource usage and enabling billing?"
  },
  {
    "question": "What is the difference between Horizontal duplication and Functional decomposition in microservices?",
    "options": [
      {
        "text": "Horizontal duplication is about scaling horizontally by replicating the whole application. Functional decomposition is about organising the application in functionalities that could be scaled individually according to the function's workload",
        "image": ""
      },
      {
        "text": "Horizontal duplication is about replicating the whole application to improve availability. Functional decomposition is about partitioning the workload among replicas of the same function",
        "image": ""
      },
      {
        "text": "Horizontal duplication is about scaling horizontally. Functional decomposition is about to replicate a functionality and to assign a different data partition to each replica",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Horizontal duplication (or horizontal scaling) means replicating the entire application across multiple instances to handle more load. Functional decomposition means breaking the application into separate services based on business capabilities, where each service can be scaled independently based on its own workload patterns.",
    "hint": "Think about what 'horizontal' means in scaling (replicating whole units) versus separating the application into distinct functional units."
  },
  {
    "question": "The microservices architecture is defined as “an architectural style that structures an application as a collection of services that are: highly maintainable and testable; loosely coupled; Independently deployable; organized around business capabilities; owned by a small team.” Why services should be loosely coupled?",
    "options": [
      {
        "text": "To facilitate the use of third party services",
        "image": ""
      },
      {
        "text": "To facilitate interoperability",
        "image": ""
      },
      {
        "text": "To minimize dependencies",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Loose coupling minimizes dependencies between services, allowing each service to evolve independently, be deployed separately, and reducing the blast radius when changes are made.",
    "hint": "Consider what happens when services are tightly coupled - a change in one service could break others."
  },
  {
    "question": "Let us consider an independent components event based system. What a subscriber component should provide?",
    "options": [
      {
        "text": "A callback function that will be executed to generate events",
        "image": ""
      },
      {
        "text": "An application programming interface to query for a specifi event",
        "image": ""
      },
      {
        "text": "A callback function that will be executed when the event is activated",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In an event-based system, the subscriber registers a callback function that gets invoked when the specific event is published/triggered, which is the fundamental publish-subscribe pattern.",
    "hint": "A subscriber 'subscribes' to events and needs a way to be notified when those events occur."
  },
  {
    "question": "Linux Namespace is an enabling technology for containers. What is exactly a Namespace?",
    "options": [
      {
        "text": "A namespace is a Linux kernel feature that allow to identify a set of  local resource belonging (visible) to the processes within the namespace",
        "image": ""
      },
      {
        "text": "A namespace is a Linux kernel feature which wraps a global system resource in an abstraction and allow to limit and monitor the usage of varius type of resources",
        "image": ""
      },
      {
        "text": "A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "A namespace wraps a global system resource in an abstraction that makes processes within the namespace believe they have their own isolated instance of that resource, enabling container isolation.",
    "hint": "The key concept is that it makes resources appear as isolated instances to processes inside the namespace."
  },
  {
    "question": "Consider the docker CLI command `docker run -dp 3000:3000 getting-started`, which of the following actions is performed by the command?",
    "options": [
      {
        "text": "Run and duplicate the getting-started container (-d) and create a mapping between the host's port 3000 to the container's port 3000 (-p)",
        "image": ""
      },
      {
        "text": "Run and duplicate the getting-started container (-d) and create a mapping between the ports 3000 of the two containers (-p)",
        "image": ""
      },
      {
        "text": "Run the getting-started container in the background (-d) and create a mapping between the host's port 3000 to the container's port 3000 (-p)",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The -d flag runs the container in detached mode (in the background), while -p 3000:3000 maps host port 3000 to container port 3000. There is no duplication involved.",
    "hint": "Remember what -d stands for in docker and that -p creates port mapping between host and container, not between containers."
  },
  {
    "question": "A Bigtable is a sparse, distributed, persistent multidimensional sorted map. Which are the keys needed to locate a desired cell's version?",
    "options": [
      {
        "text": "Row key, column key and timestamp",
        "image": ""
      },
      {
        "text": "Row key, family key and timestamp",
        "image": ""
      },
      {
        "text": "Only a row key",
        "image": ""
      },
      {
        "text": "Row key and column key",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "In Bigtable, data is organized as a sorted map indexed by three dimensions: row key, column key, and timestamp. The timestamp is essential because Bigtable stores multiple versions of each cell, and each version is identified by its timestamp.",
    "hint": "Remember that Bigtable supports versioning, so you need the timestamp to identify which version of the cell you want to access."
  },
  {
    "question": "A Bigtable uses timestamp to maintain versioning of the data stored. What of the followings is a mechanism to garbage collect cell versions automatically?",
    "options": [
      {
        "text": "Only the last n versions of a cell be kept",
        "image": ""
      },
      {
        "text": "Only last 3 versions of a cell be kept",
        "image": ""
      },
      {
        "text": "Only the last 7 days versions be kept",
        "image": ""
      },
      {
        "text": "Maximum the last n versions that are new-enough versions be kept",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Bigtable provides automatic garbage collection of old cell versions through a configurable policy. The most common approach is to specify the maximum number of versions to keep for each cell, allowing the system to automatically delete older versions.",
    "hint": "Think about what 'garbage collection' means in terms of removing old data automatically based on some criterion."
  },
  {
    "question": "Researchers and engineers at Google designed the Google File System taking into account the file characteristics. What is the typical range for file size?",
    "options": [
      {
        "text": "Hundreds of Megabyte to few Terabytes",
        "image": ""
      },
      {
        "text": "Few Megabyte to hundreds of Gigabytes",
        "image": ""
      },
      {
        "text": "Kilobyte to few Gigabytes",
        "image": ""
      },
      {
        "text": "Few Gigabyte to hundreds of Terabytes",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "The Google File System was designed specifically for large files, which is why it optimizes for files that are typically very large, ranging from a few gigabytes to hundreds of terabytes.",
    "hint": "Consider what type of workloads Google was optimizing for when designing GFS."
  },
  {
    "question": "What is the chunk size in the Google File System?",
    "options": [
      {
        "text": "64 MegaByte",
        "image": ""
      },
      {
        "text": "128 MegaByte",
        "image": ""
      },
      {
        "text": "1 GigaByte",
        "image": ""
      },
      {
        "text": "Configurable from 64 Megabyte to 128 Megabyte",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "GFS uses a fixed chunk size of 64 Megabytes, which was chosen to reduce the number of interactions with the master server and to minimize metadata overhead.",
    "hint": "This was a design choice to balance between metadata management and I/O efficiency."
  },
  {
    "question": "Consider the architecture of the HDFS in the figure. Which are the main responsibilities of the NameNode?",
    "options": [
      {
        "text": "Manages the File System Namespace; store metadata; check DataNode liveness",
        "image": ""
      },
      {
        "text": "To write in a file; store metadata; check DataNode liveness",
        "image": ""
      },
      {
        "text": "To manage the cryptographic keys; manages the File System Namespace; check datanode liveness",
        "image": ""
      },
      {
        "text": "To handle the communication with the data nodes; manages the File System Namespace; store metadata",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The NameNode is the master node in HDFS responsible for maintaining the file system namespace, storing all metadata about files and directories, and monitoring the health of DataNodes through heartbeat messages.",
    "hint": "Think about which node in HDFS manages the directory structure and monitors cluster health."
  },
  {
    "question": "Consider the Big Table datastore in the figure. Which among the followings are responsibility of the Master node?",
    "options": [
      {
        "text": "To assign tablets to tablet servers; to detect the addition and expiration of tablet servers; to balance tablet-server load, and garbage collection of files in GFS",
        "image": ""
      },
      {
        "text": "To handle read and write requests to the tablet servers; to assign tablets to tablet server; to detect the addition and expiration of tablet servers",
        "image": ""
      },
      {
        "text": "To identify tablet that have grown too large and that need to be split; to balance tablet-server load and garbage collection of files in GFS; to assign tablets to tablet servers",
        "image": ""
      },
      {
        "text": "To manage the tables in the tablet servers; to detect the addition and expiration of tablet servers; to balance tablet-server load, and garbage collection of files in GFS",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The Master node in BigTable handles metadata operations like assigning tablets to servers, detecting server changes, load balancing, and garbage collection. The actual read/write operations are handled by tablet servers, not the Master.",
    "hint": "Think about what coordinates the system versus what handles the actual data operations."
  },
  {
    "question": "A Bigtable is a sparse, distributed, persistent multidimensional sorted map. Which are the keys needed to locate a desired cell?",
    "options": [
      {
        "text": "Row key, column key and timestamp",
        "image": ""
      },
      {
        "text": "Row key, family key and timestamp",
        "image": ""
      },
      {
        "text": "Only a row key",
        "image": ""
      },
      {
        "text": "Row key and column key",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "While BigTable stores data with timestamps for versioning, the essential keys needed to locate a cell are the row key and column key. Timestamps are optional parameters used to retrieve specific versions.",
    "hint": "Consider what's minimally required to uniquely identify a cell versus what's used for version control."
  },
  {
    "question": "Consider the docker CLI command `docker build -t getting-started` which of the following actions is performed by the command?",
    "options": [
      {
        "text": "Build a container processing a Dockerfile stored in the current working directory and tag the new container with the name getting-started",
        "image": ""
      },
      {
        "text": "Build a container processing a Dockerfile named getting-started",
        "image": ""
      },
      {
        "text": "Build a container processing a Dockerfile named getting-started and store it in the current working directory rather then uploading it in the docker registry",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The -t flag tags the built image with the name 'getting-started'. When no Dockerfile path is specified, Docker looks for it in the current working directory by default.",
    "hint": "Remember the distinction between image tagging and specifying the Dockerfile location."
  },
  {
    "question": "How data are typically handled in a microservice architecture?",
    "options": [
      {
        "text": "Services are stateless, and then do not maintain any data model",
        "image": ""
      },
      {
        "text": "Each service has its own database and data model",
        "image": ""
      },
      {
        "text": "Services share a global data model and a databases",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In microservice architecture, each service should have its own database and data model to ensure loose coupling and independence. Sharing databases would create tight coupling between services.",
    "hint": "Think about what enables services to be independently deployable and loosely coupled."
  },
  {
    "question": "Which of the following is a challenges in the microservice architecture?",
    "options": [
      {
        "text": "Experimenting and adoption of new technologies;",
        "image": ""
      },
      {
        "text": "Fault isolation",
        "image": ""
      },
      {
        "text": "Deploying features that span multiple services developed by different teams",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Deploying features that span multiple services developed by different teams requires significant coordination and is challenging due to the distributed nature of microservices.",
    "hint": "Consider which challenge arises specifically from the distributed and decentralized nature of microservices."
  },
  {
    "question": "Which of the following is a key difference between microservices and SOA?",
    "options": [
      {
        "text": "Programming languages",
        "image": ""
      },
      {
        "text": "Data security",
        "image": ""
      },
      {
        "text": "Data handling",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In microservices architecture, each service typically owns its own data store (database per service), whereas SOA often uses a more centralized or shared data layer. This decentralized data ownership is a key distinction.",
    "hint": "Think about where data is stored - locally with each service or in a shared/common location."
  },
  {
    "question": "Which of the following is a benefit of microservice architecture?",
    "options": [
      {
        "text": "The microservice architecture enables fault propagation",
        "image": ""
      },
      {
        "text": "The microservice architecture enable small and easily maintained services.",
        "image": ""
      },
      {
        "text": "The microservice architecture enables intensive collaboration among teams",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Microservices are designed to be small, focused, and independently deployable units of functionality, making them easier to develop, test, and maintain compared to monolithic applications.",
    "hint": "Consider what the 'micro' prefix emphasizes in the architecture name."
  },
  {
    "question": "_____________ is a global description of the participating services, which is defined by the exchange of",
    "options": [
      {
        "text": "Service choreography",
        "image": ""
      },
      {
        "text": "Service orchestration",
        "image": ""
      },
      {
        "text": "Service automation",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Service choreography describes a decentralized approach where services interact through a global, shared understanding of the exchange protocol, without a central orchestrator controlling the flow.",
    "hint": "Recall the difference between a conductor (orchestration) and a dance where participants coordinate themselves (choreography)."
  },
  {
    "question": "Suppose you want to add a new record in a database using a REST web service. Are the following HTTP requests equivalent?",
    "options": [
      {
        "text": "YES;",
        "image": ""
      },
      {
        "text": "NO",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Both POST and PUT HTTP methods can be used to create new resources in a RESTful service. While they have different semantic meanings (POST for creation, PUT for replacement), both can add a new record to the database.",
    "hint": "Think about which HTTP methods are commonly used for resource creation in REST APIs."
  },
  {
    "question": "Suppose you must store the container's data on a remote host or a cloud provider rather than locally. Which Docker storage option is the most appropriate?",
    "options": [
      {
        "text": "Tmpfs mount;",
        "image": ""
      },
      {
        "text": "Bind mount",
        "image": ""
      },
      {
        "text": "Volumes",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Docker volumes are specifically designed for persistent data storage and can be stored on remote hosts, cloud providers, or local storage, making them the appropriate choice for persisting container data externally.",
    "hint": "Consider which Docker storage option is meant specifically for persistent data that outlives the container lifecycle."
  },
  {
    "question": "Assume you are setting up a scalable web server with AWS and configuring a step scaling policy for your autoscaling group. You have set the alarm to start the scale-out with a threshold on the average value of the incoming traffic. Which of the following assertions is true?",
    "options": [
      {
        "text": "The alarm occurs if, for the majority of the instances in the scaling group, the value of the incoming traffic exceeds the threshold (e.g. for 3 out of 5).",
        "image": ""
      },
      {
        "text": "The alarm occurs if there is at least an instance in the scaling group with the value of the incoming traffic exceeding the threshold.",
        "image": ""
      },
      {
        "text": "The alarm occurs if the average value of the incoming traffic computed over all the instances in the scaling group exceed the threshold.",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In AWS Auto Scaling, when you configure an alarm on the 'average' value of incoming traffic, AWS calculates the arithmetic mean across ALL instances in the Auto Scaling Group. If this average exceeds the threshold, the alarm triggers a scaling action.",
    "hint": "Remember that 'average' means sum of all values divided by number of instances, not a majority or any single instance."
  },
  {
    "question": "Consider the Kubernetes architecture in the figure. Assume you configure the pod autoscaling mechanisms. What is the missing component in the figure?",
    "options": [
      {
        "text": "The planner module",
        "image": ""
      },
      {
        "text": "The analyzer module",
        "image": ""
      },
      {
        "text": "A resource usage monitor module",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In Kubernetes Horizontal Pod Autoscaling, a metrics collector (like metrics-server) is needed to gather resource usage data from pods. This monitoring component provides the data that the autoscaler uses to make scaling decisions.",
    "hint": "Autoscaling decisions require real-time data about pod resource consumption - what component gathers this?"
  },
  {
    "question": "An autoscaling algorithm is considered proactive when the number of needed resources is computed using an ML or mathematical model.",
    "options": [
      {
        "text": "TRUE",
        "image": ""
      },
      {
        "text": "FALSE",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Proactive autoscaling uses predictive models (ML or mathematical) to forecast future load and provision resources in advance. Reactive autoscaling responds to current conditions. The statement actually describes proactive correctly, but the expected answer per course materials is FALSE (possibly referring to reactive/feedback-based approaches).",
    "hint": "Consider the difference between responding to current metrics vs predicting future needs."
  },
  {
    "question": "___________________ is the ability to constant monitoring for optimal operation",
    "options": [
      {
        "text": "Self-optimization",
        "image": ""
      },
      {
        "text": "Self-healing",
        "image": ""
      },
      {
        "text": "Self-configuration",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Self-optimization refers to the capability of a system to continuously monitor its own performance and automatically adjust parameters to maintain optimal operation. It's about ongoing refinement based on observed metrics.",
    "hint": "The key word is 'constant monitoring' combined with making adjustments for 'optimal operation'."
  },
  {
    "question": "Who is responsible for the Security IN the cloud?",
    "options": [
      {
        "text": "The Cloud Service Provider",
        "image": ""
      },
      {
        "text": "The Customer",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In the AWS shared responsibility model, the customer is responsible for security 'IN' the cloud (their data, applications, access management, etc.), while the provider is responsible for security 'OF' the cloud (the underlying infrastructure).",
    "hint": "The provider secures the infrastructure; you secure what you put in the cloud."
  },
  {
    "question": "Consider you want to set up an Elastic Load Balancer for supporting HTTPS requests. Which type of ELB will you select?",
    "options": [
      {
        "text": "Network load balancer",
        "image": ""
      },
      {
        "text": "Application Load Balancer",
        "image": ""
      },
      {
        "text": "Classic Load Balancer",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Application Load Balancer (ALB) operates at Layer 7 (application layer) of the OSI model, which is required for handling HTTPS traffic with SSL termination and advanced routing features like path-based routing. Network Load Balancer works at Layer 4 (transport layer) and doesn't natively handle HTTP/HTTPS protocols at the application level.",
    "hint": "Consider which OSI layer handles HTTPS protocol and routing decisions based on URL paths."
  },
  {
    "question": "Data replication across Regions is controlled by ______________",
    "options": [
      {
        "text": "AWS",
        "image": ""
      },
      {
        "text": "Customer",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In the AWS shared responsibility model, AWS provides the infrastructure and regional capabilities, but the customer is responsible for configuring and managing data replication across Regions, including choosing the replication strategy and implementing it.",
    "hint": "Think about what AWS provides versus what customers must configure themselves in shared responsibility model."
  },
  {
    "question": "When will the public IPv4 address and external DNS hostname of an AWS EC2 instance change?",
    "options": [
      {
        "text": "When an instance is launched",
        "image": ""
      },
      {
        "text": "When an instance is stopped and then started again",
        "image": ""
      },
      {
        "text": "When an instance is rebooted",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "When an EC2 instance is stopped and started again, it's essentially provisioned on new underlying hardware, which assigns a new public IPv4 address. Rebooting the instance doesn't change the IP address since it keeps the same underlying host.",
    "hint": "Consider what happens to the underlying physical hardware when an instance is stopped and started versus rebooted."
  },
  {
    "question": "Why scalability is related to performance in a cloud system?",
    "options": [
      {
        "text": "Scalability measure how fast and efficiently a system can complete certain task",
        "image": ""
      },
      {
        "text": "Scalability measure how fast is an elasticity mechanism in provisioning or deprovisioning resources",
        "image": ""
      },
      {
        "text": "Scalability measure the trend of performance with increasing load",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Scalability in cloud systems measures how performance (such as throughput, latency, or response time) changes as the workload or load increases. It relates to performance by showing whether a system can maintain performance levels when demand grows.",
    "hint": "Think about the relationship between increasing workload and system performance metrics."
  },
  {
    "question": "Which type of scalability is facilitated by microservices",
    "options": [
      {
        "text": "Scale by cloning",
        "image": ""
      },
      {
        "text": "Single functions scalability",
        "image": ""
      },
      {
        "text": "Scaling of Application data",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Microservices architecture allows each individual service or function to be scaled independently based on its specific demand, rather than scaling the entire application. This is called single function scalability because each microservice can be replicated or modified separately.",
    "hint": "Consider how microservices architecture differs from monolithic architecture in terms of independent component scaling."
  },
  {
    "question": "Which of the following is a benefit of microservice architecture?",
    "options": [
      {
        "text": "The microservice architecture increases the complexity of service maintenance",
        "image": ""
      },
      {
        "text": "The microservice architecture enables intensive collaboration among teams",
        "image": ""
      },
      {
        "text": "The microservice architecture enables fault isolation",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Il beneficio principale dell'architettura a microservizi è l'isolamento dei guasti: se un servizio fallisce, gli altri continuano a funzionare autonomamente, limitando l'impatto dell'errore.",
    "hint": "Pensare a come un errore in un componente monolite può bloccare l'intera applicazione."
  },
  {
    "question": "Suppose you must back up, restore, or migrate data from one Docker host to another. Which Docker storage option is the most appropriate?",
    "options": [
      {
        "text": "Tmpfs mount",
        "image": ""
      },
      {
        "text": "Volumes",
        "image": ""
      },
      {
        "text": "Bind mount",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "I Docker Volumes sono lo storage raccomandato per persistenza dati perché sono gestiti da Docker e possono essere facilmente condivisi, backuppati e migrati tra host diversi.",
    "hint": "Quale opzione è progettata per essere indipendente dal ciclo di vita del container?"
  },
  {
    "question": "___________________ is the ability to constant monitoring for optimal operation",
    "options": [
      {
        "text": "Self-optimization",
        "image": ""
      },
      {
        "text": "Self-healing",
        "image": ""
      },
      {
        "text": "Self-configuration",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Self-optimization è la capacità di monitorare costantemente le prestazioni e ottimizzare automaticamente le risorse per mantenere un funzionamento ottimale del sistema.",
    "hint": "Quale termine descrive il miglioramento continuo basato su metriche di monitoraggio?"
  },
  {
    "question": "Suppose you want to add a new record in a database using a REST web service. Are the following HTTP requests equivalent?",
    "options": [
      {
        "text": "YES;",
        "image": ""
      },
      {
        "text": "NO",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "I metodi HTTP hanno semantica diversa: POST crea nuove risorse, PUT le sostituisce, GET recupera. Non sono equivalenti per aggiungere record a un database.",
    "hint": "La semantica dei verbi HTTP definisce il comportamento atteso del server."
  },
  {
    "question": "How can be classified a cloud mechanism that guarantees a given number of VMs or containers are always available (up & running)?",
    "options": [
      {
        "text": "Self-protecting mechanism",
        "image": ""
      },
      {
        "text": "Self-optimization mechanism",
        "image": ""
      },
      {
        "text": "Self-healing mechanism",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Self-healing è la capacità di rilevare automaticamente guasti e ripristinare lo stato desiderato, garantendo che un dato numero di istanze rimanga sempre disponibile.",
    "hint": "Quale meccanismo si occupa di mantenere attive le risorse nonostante i fallimenti?"
  },
  {
    "question": "An AWS Region typically consists of two or more ___________________.",
    "options": [
      {
        "text": "Storage replicas",
        "image": ""
      },
      {
        "text": "Availability Zones",
        "image": ""
      },
      {
        "text": "Datacenters",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "AWS Regions are composed of multiple Availability Zones (AZs), which are physically separated data centers within a geographic area. This architecture provides high availability and fault tolerance by allowing applications to be distributed across isolated infrastructure.",
    "hint": "Think about how AWS provides fault isolation and high availability within a geographic region."
  },
  {
    "question": "Who is responsible for the Security OF the cloud?",
    "options": [
      {
        "text": "The Cloud Service Provider",
        "image": ""
      },
      {
        "text": "The Customer",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "In the AWS shared responsibility model, the Cloud Service Provider is responsible for the security OF the cloud - meaning the physical infrastructure, virtualization layer, and underlying services. The customer is responsible for security IN the cloud (data, access, applications).",
    "hint": "Remember the distinction between 'security OF the cloud' (provider's responsibility) and 'security IN the cloud' (customer's responsibility)."
  },
  {
    "question": "Which of the following is a benefit of microservice architecture?",
    "options": [
      {
        "text": "The microservice architecture allows easy experimenting and adoption of new technologies",
        "image": ""
      },
      {
        "text": "The microservice architecture enables intensive collaboration among teams",
        "image": ""
      },
      {
        "text": "The microservice architecture enables fault propagation",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Microservice architecture allows each service to be built with different technologies and updated independently, making it easy to experiment with new technologies and adopt the best tools for specific tasks without affecting the entire application.",
    "hint": "Consider how microservices enable technology independence between different parts of an application."
  },
  {
    "question": "Suppose you must share configuration files from the host machine to containers . Which Docker storage option is the most appropriate?",
    "options": [
      {
        "text": "Tmpfs mount",
        "image": ""
      },
      {
        "text": "Volumes",
        "image": ""
      },
      {
        "text": "Bind mount",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Bind mounts mount a specific host file or directory directly into a container, making them the ideal choice for sharing configuration files from the host machine to containers at runtime.",
    "hint": "Which Docker storage option directly maps a host filesystem path into the container?"
  },
  {
    "question": "Consider setting up an Elastic Load Balancer to support traffic to a containerized application. Which type of ELB will you select?",
    "options": [
      {
        "text": "Application Load Balancer",
        "image": ""
      },
      {
        "text": "Network load balancer",
        "image": ""
      },
      {
        "text": "Classic Load Balancer",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Application Load Balancers (ALBs) are designed for containerized applications and support dynamic port mapping, allowing routing to multiple containers on the same EC2 instance, which is essential for container orchestration.",
    "hint": "Which load balancer type is specifically optimized for container-based architectures with dynamic port mapping?"
  },
  {
    "question": "Consider setting up an Elastic Load Balancer for supporting a static or Elastic IP address, or an IP target outside a VPC. Which type of ELB will you select?",
    "options": [
      {
        "text": "Network load balancer",
        "image": ""
      },
      {
        "text": "Application Load Balancer",
        "image": ""
      },
      {
        "text": "Classic Load Balancer",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Network Load Balancer operates at layer 4 (TCP/UDP) and provides static IP addresses that can be associated with Elastic IPs. Unlike Application Load Balancer (layer 7), NLB supports IP targets and can route traffic to resources outside the VPC.",
    "hint": "Think about which load balancer operates at transport layer and supports IP-based routing."
  },
  {
    "question": "Each ___________________ is a fully isolated partition of the AWS infrastructure.",
    "options": [
      {
        "text": "Datacenter",
        "image": ""
      },
      {
        "text": "Availability Zone",
        "image": ""
      },
      {
        "text": "Region",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "An Availability Zone is a physically isolated data center within an AWS region, with its own power, cooling, and networking. This isolation provides fault tolerance - failures in one AZ don't affect others.",
    "hint": "It's the smallest isolated unit within an AWS region."
  },
  {
    "question": "Which of the following is a challenges in the microservice architecture?",
    "options": [
      {
        "text": "Experimenting and adoption of new technologies;",
        "image": ""
      },
      {
        "text": "Fault isolation",
        "image": ""
      },
      {
        "text": "Development, testing and deployment is challenging",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Microservices introduce complexity in development workflows, testing across service boundaries, and coordinating deployments. Managing multiple independent services requires sophisticated CI/CD pipelines and poses operational challenges compared to monolithic architectures.",
    "hint": "Consider the coordination overhead inherent in distributed systems."
  },
  {
    "question": "Suppose you must share source code or build artifacts between a development environment on the Docker host and a container. Which Docker storage option is most appropriate?",
    "options": [
      {
        "text": "Tmpfs mount",
        "image": ""
      },
      {
        "text": "Volumes",
        "image": ""
      },
      {
        "text": "Bind mount",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Bind mount maps a host directory or file directly into the container, allowing real-time sharing of source code and build artifacts between the Docker host and container. This is ideal for development workflows where changes need immediate visibility.",
    "hint": "Which storage option directly exposes host filesystem paths to the container?"
  },
  {
    "question": "An autoscaling algorithm is considered proactive when the scaling decision is taken on the basis of a predicted value of the scaling metrics. E.g. predicted value of CPU utilization in the next 5 minutes.",
    "options": [
      {
        "text": "TRUE",
        "image": ""
      },
      {
        "text": "FALSE",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "A proactive autoscaling algorithm uses predictive analytics to forecast future metric values (like CPU utilization) and makes scaling decisions in advance, rather than reacting to current conditions.",
    "hint": "The keyword is 'predicted value' - what does this tell you about the timing of the scaling decision?"
  },
  {
    "question": "Each AWS Region provides ____________________ and connectivity to the network.",
    "options": [
      {
        "text": "high availability ",
        "image": ""
      },
      {
        "text": "traffic load balancing",
        "image": ""
      },
      {
        "text": "full redundancy",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "AWS Regions are designed to be geographically isolated with completely independent infrastructure, including power, networking, and facilities, ensuring full redundancy across all components.",
    "hint": "Think about geographic isolation and independent operation of cloud infrastructure."
  },
  {
    "question": "Which among the following is the appropriate definition for capacity planning",
    "options": [
      {
        "text": "Methods, models and algorithms to provide the right amount of capacity when needed",
        "image": ""
      },
      {
        "text": "Methods models and algorithms for adding capacity to an IT resource in anticipation of demand  KO",
        "image": ""
      },
      {
        "text": "Methods. models and algorithms to scale up/down or in/out",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Capacity planning focuses on providing the right amount of resources at the right time, matching supply with demand dynamically.",
    "hint": "The emphasis is on timing - having capacity available when it's actually needed."
  },
  {
    "question": "Which of the following is a key difference between microservices and SOA?",
    "options": [
      {
        "text": "Communication mechanisms",
        "image": ""
      },
      {
        "text": "Programming languages",
        "image": ""
      },
      {
        "text": "Data security",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The key difference lies in communication: microservices use lightweight protocols like HTTP/REST, while SOA traditionally employs enterprise service buses (ESBs) with more complex messaging.",
    "hint": "Consider the difference between lightweight APIs and heavyweight enterprise messaging."
  },
  {
    "question": "_____________ represents a single centralized executable business process that coordinates the interaction among different services.",
    "options": [
      {
        "text": "Service automation",
        "image": ""
      },
      {
        "text": "Service choreography",
        "image": ""
      },
      {
        "text": "Service orchestration",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Service orchestration uses a central coordinator (like an orchestrator) that controls the workflow and invokes services in a specific sequence, similar to a conductor directing an orchestra.",
    "hint": "Think about centralized control versus distributed collaboration."
  },
  {
    "question": "___________________ is the ability to accommodate varying and possibly unpredictable conditions.",
    "options": [
      {
        "text": "Self-healing",
        "image": ""
      },
      {
        "text": "Self-configuration",
        "image": ""
      },
      {
        "text": "Self-optimization",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Self-configuration refers to a system's ability to automatically adjust and configure itself based on varying and unpredictable conditions without manual intervention.",
    "hint": "Consider what 'auto' means when conditions change unexpectedly."
  },
  {
    "question": "Which of the following is a benefit of microservice architecture?",
    "options": [
      {
        "text": "The microservice architecture enables fault propagation",
        "image": ""
      },
      {
        "text": "The microservice architecture limits adoption of new technologies",
        "image": ""
      },
      {
        "text": "The microservice architecture enables teams to be autonomous.",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Microservices allow teams to own complete services independently, enabling them to make autonomous decisions about development, testing, and deployment without coordinating with other teams.",
    "hint": "Think about how decomposition into independent services affects team organization and independence."
  },
  {
    "question": "Consider setting up an Elastic Load Balancer for handling extremely spiky and unpredictable TCP traffic. Which type of ELB will you select?",
    "options": [
      {
        "text": "Classic Load Balancer",
        "image": ""
      },
      {
        "text": "Application Load Balancer",
        "image": ""
      },
      {
        "text": "Network load balancer",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Network Load Balancer is designed for extreme performance scenarios, handling millions of requests per second with ultra-low latency, making it ideal for spiky, unpredictable TCP traffic.",
    "hint": "Consider which AWS load balancer is optimized for high-throughput, low-latency TCP connections."
  },
  {
    "question": "Which of the following is a challenges in the microservice architecture?",
    "options": [
      {
        "text": "Fault isolation",
        "image": ""
      },
      {
        "text": "Finding the right set of services",
        "image": ""
      },
      {
        "text": "Experimenting and adoption of new technologies;",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "A fundamental challenge in microservices is determining the appropriate service boundaries and granularity - deciding how to decompose the application into the right set of cohesive services.",
    "hint": "Think about what makes it difficult to initially design microservices."
  },
  {
    "question": "Assume you are setting up a scalable web server with AWS and configuring a step scaling policy for your autoscaling group. You have set the alarm to start the scale-out with a threshold on the maximum incoming traffic. Which of the following assertions is true?",
    "options": [
      {
        "text": "The alarm occurs if for all the instances in the scaling group the value of the incoming traffic exceeds the threshold.",
        "image": ""
      },
      {
        "text": "The alarm occurs if the average value of the incoming traffic computed over all the instances in the scaling group exceed the threshold.",
        "image": ""
      },
      {
        "text": "The alarm occurs if for at least one instance the maximum observed incoming traffic exceeds the threshold.",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In AWS Auto Scaling, CloudWatch alarms evaluate metrics per instance - if any single instance's metric (like maximum incoming traffic) exceeds the threshold, the alarm triggers the scale-out action.",
    "hint": "Remember how AWS evaluates instance-level metrics in autoscaling group alarms."
  }
]