[
  {
    "question": "What class does the Naive Bayes classifier predict for a given observation?",
    "options": [
      {
        "text": "The class maximizing the joint predictors probability",
        "image": ""
      },
      {
        "text": "The class minimizing the joint predictors probability",
        "image": ""
      },
      {
        "text": "The class maximizing the joint predictors/labels probability",
        "image": ""
      },
      {
        "text": "The class minimizing the joint predictors/labels probability",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Naive Bayes uses Bayes' theorem to compute the posterior P(y|x) âˆ P(x|y)P(y), and predicts the class that maximizes this joint probability P(x,y).",
    "hint": "Remember that NB maximizes the posterior, which involves the joint probability of features and labels."
  },
  {
    "question": "If your dataset has two variables ğ’™, ğ’™â€² such that ğ’™ = ğ’‚ â‹… ğ’™â€² for some constant a > 0, then you have:",
    "options": [
      {
        "text": "overfitting",
        "image": ""
      },
      {
        "text": "underfitting",
        "image": ""
      },
      {
        "text": "multicollinearity",
        "image": ""
      },
      {
        "text": "supercollinearity",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Multicollinearity occurs when predictor variables are linearly dependent, which is exactly the case when x = aÂ·x' for a > 0.",
    "hint": "Think about what happens when one predictor can be perfectly expressed as a linear function of another."
  },
  {
    "question": "A naÃ¯ve Bayes classifier can deal with previously unseen feature-label combination through:",
    "options": [
      {
        "text": "Laplacian smoothing",
        "image": ""
      },
      {
        "text": "Bootstrapping",
        "image": ""
      },
      {
        "text": "Stratified cross-validation",
        "image": ""
      },
      {
        "text": "Repeated sampling",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Laplacian smoothing adds a small constant (usually 1) to count occurrences, preventing zero probabilities for unseen feature-label combinations.",
    "hint": "What technique adds a small constant to probabilities to avoid zero counts?"
  },
  {
    "question": "For a linear regression model, the expected squared error can be decomposed in:",
    "options": [
      {
        "text": "Variance and covariance",
        "image": ""
      },
      {
        "text": "SSE and SST",
        "image": ""
      },
      {
        "text": "Underfit and overfit",
        "image": ""
      },
      {
        "text": "Bias and variance noise",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "The expected prediction error in linear regression decomposes into bias (underfitting), variance (overfitting), and irreducible noise.",
    "hint": "Think about the classic decomposition of expected error into components related to model complexity."
  },
  {
    "question": " What is the key assumption of the NaÃ¯ve Bayes Classifier?",
    "options": [
      {
        "text": "The predictors and labels are independent",
        "image": ""
      },
      {
        "text": "Each predictor follows a Gaussian distribution",
        "image": ""
      },
      {
        "text": "The predictors are independent conditionally on the label",
        "image": ""
      },
      {
        "text": "The number of predictors is at most poly(n)",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The key assumption is that features are independent of each other given the class label, which simplifies the joint probability P(x|y) to a product of individual conditional probabilities.",
    "hint": "Consider how conditional independence simplifies complex probability calculations."
  },
  {
    "question": "Which one of the following performance indicates the best model for prediction?",
    "options": [
      {
        "text": "ğ‘…' = 0.2 on training, ğ‘…' = 0.1 on test",
        "image": ""
      },
      {
        "text": "ğ‘…' = 0.7 on training, ğ‘…' = 0.7 on test",
        "image": ""
      },
      {
        "text": "ğ‘…' = 0.8 on training, ğ‘…' = 0.1 on test",
        "image": ""
      },
      {
        "text": "ğ‘…' = 0.9 on training, ğ‘…' = âˆ’0.9 on test",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "A good predictive model should generalize well, meaning it performs similarly on both training and test data. Option B shows consistent performance (0.7 on both), indicating the model has learned generalizable patterns without overfitting.",
    "hint": "Look for the model that performs similarly on both training and test data, not just high on one."
  },
  {
    "question": "You want to predict the market price of a teamâ€™s merchandising (t-shirts, hats..), according to the teamâ€™s seasonal performance. You suggest using:",
    "options": [
      {
        "text": "Linear regression",
        "image": ""
      },
      {
        "text": "Logistic regression",
        "image": ""
      },
      {
        "text": "Linear programming",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Since we want to predict a continuous value (market price) based on another variable (seasonal performance), this is a regression problem. Linear regression is specifically designed to predict continuous outcomes.",
    "hint": "Consider whether you need to predict continuous values or discrete categories."
  },
  {
    "question": "When is the accuracy a misleading classifier performance measure?",
    "options": [
      {
        "text": "When the population label proportions are unbalanced",
        "image": ""
      },
      {
        "text": "When the population label proportions are balanced",
        "image": ""
      },
      {
        "text": "When the sensitivity is high",
        "image": ""
      },
      {
        "text": "When the specificity is low",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "With imbalanced class distributions, a classifier can achieve high accuracy by simply predicting the majority class most of the time, which masks poor performance on the minority class.",
    "hint": "Think about what happens when one class is much more frequent than another."
  },
  {
    "question": "The goal of linear regression is to?",
    "options": [
      {
        "text": "Make America great again",
        "image": ""
      },
      {
        "text": "Group similar observations together",
        "image": ""
      },
      {
        "text": "Learn a linear function from data",
        "image": ""
      },
      {
        "text": "Evaluate the amount of noise in the data",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Linear regression's objective is to find the best-fitting linear function that describes the relationship between input variables and the continuous target variable.",
    "hint": "Regression in statistics involves fitting a function to predict continuous outcomes."
  },
  {
    "question": "In the bias-variance decomposition of the expected squared error, what does high bias suggest?",
    "options": [
      {
        "text": "Noisy data",
        "image": ""
      },
      {
        "text": "Overfitting",
        "image": ""
      },
      {
        "text": "Underfitting",
        "image": ""
      },
      {
        "text": "Crossfitting",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "High bias in the bias-variance decomposition indicates the model is too simple to capture the underlying data patterns, resulting in underfitting and poor performance on both training and test data.",
    "hint": "Bias measures how far the model's predictions are from the true values on average."
  },
  {
    "question": "Social network users often form communities according to their tastes. If you had access to their personal data, you may verify this intuition by:",
    "options": [
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Linear programming",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Clustering is an unsupervised learning technique that groups data points into clusters based on similarity, making it ideal for discovering communities of users with similar tastes without predefined labels.",
    "hint": "Think about what technique is used to discover natural groupings in data without predefined labels."
  },
  {
    "question": "R^2 is a measure of:",
    "options": [
      {
        "text": "Reliability of predictions",
        "image": ""
      },
      {
        "text": "Goodness of fit",
        "image": ""
      },
      {
        "text": "Significance of estimates",
        "image": ""
      },
      {
        "text": "Model complexity",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "RÂ² (R-squared) measures the proportion of variance in the dependent variable that is explained by the independent variables in the regression model, indicating how well the model fits the data.",
    "hint": "RÂ² tells you how much of the variation in the outcome is explained by your model."
  },
  {
    "question": "A company wants to relate the monthly revenue to productivity parameters such as total number of working hours, etc. They could use:",
    "options": [
      {
        "text": "Linear regression",
        "image": ""
      },
      {
        "text": "Logistic regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Linear programming",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Linear regression is used when the target variable is continuous, making it suitable for predicting monthly revenue from productivity parameters like working hours.",
    "hint": "Consider whether the outcome variable (revenue) is continuous or categorical."
  },
  {
    "question": "How do you perform a linear regression in R?",
    "options": [
      {
        "text": "lm(y ~ x, data)",
        "image": ""
      },
      {
        "text": "lm(y ~ x, data, family = â€œbinomialâ€)",
        "image": ""
      },
      {
        "text": "predict(y ~ x, data)",
        "image": ""
      },
      {
        "text": "predict(y ~ x, data, binomial)",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "The lm() function in R is used for linear regression models, with the formula syntax y ~ x specifying the relationship between dependent and independent variables.",
    "hint": "Remember that lm() is the base R function for fitting linear models."
  },
  {
    "question": "Your friend proposes to cluster 300 observations by trying all possible clustering and taking the one that minimizes intra cluster variance. You observe that:",
    "options": [
      {
        "text": "This is the only possible approach",
        "image": ""
      },
      {
        "text": "This does not produce a good clustering",
        "image": ""
      },
      {
        "text": "This does require a few seconds",
        "image": ""
      },
      {
        "text": "This does require a centuries",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Enumerating all possible clusterings (partitions) for 300 observations is computationally infeasible because the number of possible partitions grows super-exponentially (Bell numbers), requiring astronomical time even with modern computers.",
    "hint": "Consider how many possible ways there are to partition 300 objects into any number of clusters."
  },
  {
    "question": "Single-linkage clustering works by",
    "options": [
      {
        "text": "Repeatedly recomputing the centroids of clusters",
        "image": ""
      },
      {
        "text": "Repeatedly merging smaller clusters into larger ones",
        "image": ""
      },
      {
        "text": "Enumerating all possible clustering of the given points",
        "image": ""
      },
      {
        "text": "Enumerating all possible points in a cluster",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Single-linkage is a hierarchical agglomerative clustering method that iteratively merges the pair of clusters with the smallest minimum distance between any two points, building larger clusters from smaller ones.",
    "hint": "Think about what 'agglomerative' means - it involves merging things together gradually."
  },
  {
    "question": "In linear regression, a high value of ğ‘¹ğŸ on the training set suggests:",
    "options": [
      {
        "text": "A small error of the model on the fitted data",
        "image": ""
      },
      {
        "text": "A small error of the model on future predictions",
        "image": ""
      },
      {
        "text": "A large error of the model on the fitted data",
        "image": ""
      },
      {
        "text": "A large error of the model on future predictions",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "RÂ² on the training set measures the proportion of variance explained by the model on the training data itself, indicating fit quality on the training set but not predictive performance on new data.",
    "hint": "Remember RÂ² is calculated on the training data, not on test/validation data."
  },
  {
    "question": "A logistic regression gives the following scores, preceded by the actual label: (Y, 0.85), (Y, 0.75), (N,0.6), (Y,0.5), (N, 0.4), (N, 0.2). For a sensitivity of at least 2/3, the best choice is to predict Y when the score is at least:",
    "options": [
      {
        "text": "0.9",
        "image": ""
      },
      {
        "text": "0.75",
        "image": ""
      },
      {
        "text": "0.6",
        "image": ""
      },
      {
        "text": "0.45",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "With sensitivity = TP/(TP+FN), we need at least 2/3. With threshold 0.75, we correctly predict Y for scores 0.85 and 0.75 (2 out of 3 Y values), achieving exactly 2/3 sensitivity.",
    "hint": "Calculate sensitivity (true positive rate) for each threshold option."
  },
  {
    "question": "Look at the confusion matrix below. What we can say?",
    "options": [
      {
        "text": "The sensitivity is < 0.80%",
        "image": ""
      },
      {
        "text": "There are less positives than negatives",
        "image": ""
      },
      {
        "text": "The accuracy is > 90%",
        "image": ""
      },
      {
        "text": "The classifier predicts 1 on 60% of the times",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "https://raw.githubusercontent.com/dag7dev/UniQuizzes/master/img/FDS/matrixwhatcanwesay.png.png",
    "code": "",
    "explanation": "The question refers to a confusion matrix where the classifier predicts the positive class (1) in 60% of cases, meaning the proportion of positive predictions equals 60%.",
    "hint": "Look at the ratio of positive predictions to total predictions in the confusion matrix."
  },
  {
    "question": "A set of observations (ğ’™ğŸ, ğ’šğŸ), (ğ’™ğŸ,ğ’šğŸ)â€¦(ğ’™ğ’, ğ’šğ’) obeys the law ğ’šğ’Š â‰” ğ’‚ğ’™ğ’Š + ğ’ƒ + ğœºğ’Š, where ğœºğ’Š is some random noise. The task of estimating a and b from the dataset is called:",
    "options": [
      {
        "text": "Logistic regression",
        "image": ""
      },
      {
        "text": "Linear regression",
        "image": ""
      },
      {
        "text": "Linear programming",
        "image": ""
      },
      {
        "text": "Logistic programming",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Fitting a linear model with parameters a and b to data following y = ax + b + noise is the definition of linear regression, which estimates these coefficients by minimizing the residual sum of squares.",
    "hint": "Consider what mathematical technique estimates parameters in a linear equation with noise."
  },
  {
    "question": "Laplacian smoothing aims at:",
    "options": [
      {
        "text": "Producing readable plots by using an average window",
        "image": ""
      },
      {
        "text": "Reducing the modelâ€™s dependence on the noise",
        "image": ""
      },
      {
        "text": "Improving the feature quality by removing outliers",
        "image": ""
      },
      {
        "text": "Avoid penalizing previously unseen observations",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Laplacian smoothing adds a constant (typically 1) to probability estimates to avoid zero probabilities for unseen events, preventing the model from excessively penalizing observations it hasn't encountered during training.",
    "hint": "Think about what happens to probabilities when a category hasn't been seen before."
  },
  {
    "question": "A dataset of points (ğ’™ğŸ, ğ’šğŸ), (ğ’™ğŸ,ğ’šğŸ)â€¦(ğ’™ğ’, ğ’šğ’) has been generated by the model ğ’šğ’Š â‰” ğ’‚ğ’™ğ’Š + ğ’ƒ + ğœºğ’Š, where ğœºğ’Š is gaussian noise. Linear regression aims at estimating:",
    "options": [
      {
        "text": "a and b",
        "image": ""
      },
      {
        "text": "a and ğœ€",
        "image": ""
      },
      {
        "text": "x and b",
        "image": ""
      },
      {
        "text": "x and y",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Linear regression estimates the parameters of the linear relationship (slope a and intercept b) that best fits the data, treating the noise as random error that should not be estimated.",
    "hint": "The noise Îµ is random and unpredictable - we don't estimate it."
  },
  {
    "question": "Which one of the following R commands selects only the rows of data where X equals 0?",
    "options": [
      {
        "text": "select(data, X == 0)",
        "image": ""
      },
      {
        "text": "filter(data, X==0)",
        "image": ""
      },
      {
        "text": "summarize(data, X==0)",
        "image": ""
      },
      {
        "text": "table(data, X == 0)",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In R's dplyr package, filter() is the function specifically designed to select rows based on a condition, while select() chooses columns and summarize() aggregates data.",
    "hint": "Which dplyr function is specifically for row selection based on conditions?"
  },
  {
    "question": "If an algorithm has exponential complexity, then we can assume that:",
    "options": [
      {
        "text": "In practice it is still fast enough to be useful",
        "image": ""
      },
      {
        "text": "It admits a polynomial-time algorithm",
        "image": ""
      },
      {
        "text": "It can be solved by finding an optimal clustering",
        "image": ""
      },
      {
        "text": "No technological progress will ever make it practical",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Exponential complexity grows so rapidly (O(2^n)) that no amount of technological improvement can make it practical for reasonably large inputs, as the growth outpaces any hardware advances.",
    "hint": "Consider how exponential growth compares to any potential future technological improvement."
  },
  {
    "question": "If you have n points, what is the number of clusters that minimizes the within-cluster sum of square?",
    "options": [
      {
        "text": "1",
        "image": ""
      },
      {
        "text": "k",
        "image": ""
      },
      {
        "text": "n",
        "image": ""
      },
      {
        "text": "We cannot say",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "With n clusters (each point in its own cluster), the within-cluster sum of squares equals zero since each centroid coincides with its point, which is the mathematical minimum.",
    "hint": "What happens when each point becomes its own cluster?"
  },
  {
    "question": "Which regression model has smaller squared error in fitting a real function ğ’‡(ğ’™)?",
    "options": [
      {
        "text": "A simple linear regression",
        "image": ""
      },
      {
        "text": "A logistic regression",
        "image": ""
      },
      {
        "text": "A polynomial regression of degree 2",
        "image": ""
      },
      {
        "text": "A polynomial regression of degree 10",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "When the true underlying function is linear (a 'real function' in the context of regression), a simple linear regression is correctly specified and will have the smallest squared error because more complex models (polynomials) would overfit or introduce unnecessary variance.",
    "hint": "Consider whether the model is correctly specified for the true function being fitted."
  },
  {
    "question": "A doping screening is tested on a pool of 800 athletes of which 796 are clean. The test is correct in 99% of the cases. What can we say about it?",
    "options": [
      {
        "text": "It may have missed all of the doped athletes",
        "image": ""
      },
      {
        "text": "It may have missed all of the clean athletes",
        "image": ""
      },
      {
        "text": "It identified all of the doped athletes",
        "image": ""
      },
      {
        "text": "It identified all of the clean atheletes",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "With only 4 doped athletes out of 800 and a 99% accuracy rate, it's possible (though unlikely) that all 4 doped athletes were misclassified as clean, meaning the test missed all of them. The 1% error rate (â‰ˆ8 people) could easily include all 4 positives.",
    "hint": "Consider the very low base rate of positives (4 out of 800) and what the 99% accuracy means for error distribution."
  },
  {
    "question": "In linear programming, the space of feasible solution is:",
    "options": [
      {
        "text": "An arbitrary set",
        "image": ""
      },
      {
        "text": "A subset of ğ‘…'",
        "image": ""
      },
      {
        "text": "A convex polytope",
        "image": ""
      },
      {
        "text": "None of the above",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "In linear programming, the feasible region is defined by linear constraints, which form a convex polytope (a bounded convex polyhedron) in â„â¿. This is a fundamental property of linear programming problems.",
    "hint": "Recall that linear constraints define half-spaces, and their intersection creates a specific geometric structure."
  },
  {
    "question": "The explained variance of a clustering equals:",
    "options": [
      {
        "text": "Within-cluster SSE divided by total sum of squares",
        "image": ""
      },
      {
        "text": "Total sum of squares divided by within-cluster SSE",
        "image": ""
      },
      {
        "text": "Within-cluster SSE divided by between-cluster SSE",
        "image": ""
      },
      {
        "text": "Total sum of squares divided by between-cluster SSE",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Explained variance in clustering measures the proportion of total variance accounted for by the cluster structure, calculated as within-cluster SSE divided by total sum of squares (1 minus this ratio gives the unexplained variance).",
    "hint": "Think of this as the complement of the unexplained variance ratio."
  },
  {
    "question": "Gradient descent is a technique we have used to:",
    "options": [
      {
        "text": "Compute the optimal number of clusters",
        "image": ""
      },
      {
        "text": "Reduce the noise in the training set",
        "image": ""
      },
      {
        "text": "Find the local minima of a function",
        "image": ""
      },
      {
        "text": "Estimate the probability of false positive",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Gradient descent is an optimization algorithm that iteratively moves in the direction of the negative gradient to find local minima of a function, commonly used in machine learning to minimize loss functions.",
    "hint": "Remember it searches for optima by following the slope of the function."
  },
  {
    "question": "Which of these models is probably overfitting?",
    "options": [
      {
        "text": "ğ‘…' = 0.1 on training, ğ‘…' = 0.1 on test",
        "image": ""
      },
      {
        "text": "ğ‘…' = 0.8 on training, ğ‘…' = 0.7 on test",
        "image": ""
      },
      {
        "text": "ğ‘…' = 0.7 on training, ğ‘…' = 0.7 on test",
        "image": ""
      },
      {
        "text": "ğ‘…' = 0.8 on training, ğ‘…' = 0.1 on test",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Overfitting occurs when a model performs very well on training data but poorly on test data, indicating it has memorized training examples rather than learning generalizable patterns. The large gap between training (0.8) and test (0.1) performance is the telltale sign of overfitting.",
    "hint": "Look for a large discrepancy between training and test performance - that's the key indicator."
  },
  {
    "question": "Laplacian smoothing aims at:",
    "options": [
      {
        "text": "Improving the feature quality by removing outliers",
        "image": ""
      },
      {
        "text": "Producing readable plots",
        "image": ""
      },
      {
        "text": "Reducing the modelâ€™s dependence on the noise",
        "image": ""
      },
      {
        "text": "Avoid penalizing previously unseen observations",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Laplacian smoothing adds a small constant (typically 1) to each count to avoid zero probabilities. This reduces the model's sensitivity to noise in the data, especially for rare events or small sample sizes.",
    "hint": "Think about what happens when you have zero or very small counts in your probability calculations."
  },
  {
    "question": "To visualize a hierarchical clustering one can use:",
    "options": [
      {
        "text": "a dendrogram ",
        "image": ""
      },
      {
        "text": "a ROC curve",
        "image": ""
      },
      {
        "text": "a boxplot",
        "image": ""
      },
      {
        "text": "a histogram",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "A dendrogram is a tree-like diagram that displays the hierarchical clustering process, showing how clusters are nested and merged at different similarity levels.",
    "hint": "Think about what structure naturally represents hierarchy - a tree-like branching structure."
  },
  {
    "question": "The goal of linear regression is to:",
    "options": [
      {
        "text": "bring peace to the world",
        "image": ""
      },
      {
        "text": "group similar observations together",
        "image": ""
      },
      {
        "text": "learn a linear function from data ",
        "image": ""
      },
      {
        "text": "evaluate the amount of noise in the data",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Linear regression finds the best-fit line (or hyperplane in higher dimensions) that minimizes the sum of squared differences between predicted and actual values, learning a linear function from the data.",
    "hint": "What mathematical object describes a straight line relationship between variables?"
  },
  {
    "question": "Naive Bayes classiers work well for:",
    "options": [
      {
        "text": "linear programming",
        "image": ""
      },
      {
        "text": "spam filtering ",
        "image": ""
      },
      {
        "text": "k-center clustering",
        "image": ""
      },
      {
        "text": "speech recognition",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Naive Bayes classifiers are particularly effective for text classification tasks like spam filtering because they handle high-dimensional sparse data well and make strong independence assumptions that work reasonably well in practice.",
    "hint": "Think about a classic, widely-used application where you need to classify text documents into categories."
  },
  {
    "question": "The explained variance of a clustering equals:",
    "options": [
      {
        "text": "(total variance)/(within-cluster variance)",
        "image": ""
      },
      {
        "text": "(within-cluster variance)/(between-cluster variance)",
        "image": ""
      },
      {
        "text": "(between-cluster variance)/(total variance) ",
        "image": ""
      },
      {
        "text": "(within-cluster variance)/(total variance)",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The explained variance in clustering measures the proportion of total variance accounted for by the cluster structure. It's computed as between-cluster variance divided by total variance (which equals within-cluster variance + between-cluster variance), similar to RÂ² in regression.",
    "hint": "Think of total variance as sum of within-cluster and between-cluster variance."
  },
  {
    "question": "A binary classifier on 6 points gives the probabilities: 0.9, 0.85, 0.75, 0.5, 0.4, 0.3; the correct labels are 1,1,0,1,0,0. What is the best probability threshold, if we need FPR <= 1/3?",
    "options": [
      {
        "text": "0.45 ",
        "image": ""
      },
      {
        "text": "1.0",
        "image": ""
      },
      {
        "text": "0.95",
        "image": ""
      },
      {
        "text": "0.25",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "FPR = FP/(FP+TN). At threshold 0.45, we get exactly 1 FP out of 3 negatives (the point with probability 0.75), giving FPR = 1/3 which satisfies the constraint.",
    "hint": "Calculate FPR for each candidate threshold by counting false positives and true negatives."
  },
  {
    "question": "Mark the wrong statement about gradient descent:",
    "options": [
      {
        "text": "batch gradient descent approximates â–½f using a mini-batch",
        "image": ""
      },
      {
        "text": "stochastic gradient descent approximates â–½f with a single example",
        "image": ""
      },
      {
        "text": "there is no guarantee to nd the global minimum",
        "image": ""
      },
      {
        "text": "increasing the learning rate damps oscillations ",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "A larger learning rate can cause more severe oscillations or instability, not damp them. Too high a learning rate causes overshooting. Batch gradient descent uses ALL samples, not mini-batch.",
    "hint": "Consider how learning rate affects the step size in the parameter update."
  },
  {
    "question": "Which task does not require to learn a model?",
    "options": [
      {
        "text": "Clustering ",
        "image": ""
      },
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Classication",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Clustering is an unsupervised learning task that groups data without labeled responses. The other methods (regression, classification) learn predictive models from labeled training data.",
    "hint": "Supervised methods require labeled data to learn a model."
  },
  {
    "question": "For two sets A, B the probability that the first element in a random permutation of A U B is in A âˆ© B:",
    "options": [
      {
        "text": "is J(A,B) / |A âˆ© B|",
        "image": ""
      },
      {
        "text": "is J(A,B) ",
        "image": ""
      },
      {
        "text": "is 1/|A|+1/|B|",
        "image": ""
      },
      {
        "text": "is 1/(|A||B|)",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "The probability equals |A âˆ© B|/|A âˆª B|, which is precisely the Jaccard index J(A,B). A random permutation gives each element equal chance to be first.",
    "hint": "This is exactly the definition of Jaccard similarity."
  },
  {
    "question": "The R^2 and the p-values of a regression:",
    "options": [
      {
        "text": "are always equivalent",
        "image": ""
      },
      {
        "text": "cannot be both positive",
        "image": ""
      },
      {
        "text": "measure different aspects ",
        "image": ""
      },
      {
        "text": "are negatively correlated",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "RÂ² measures the proportion of variance explained by the regression model (goodness of fit), while p-values test the statistical significance of the relationship between variables. They capture different aspects of model quality.",
    "hint": "Think about what each metric actually measures in terms of statistical inference vs. explanatory power."
  },
  {
    "question": "xXmini=1k||x-ci||22 is the objective function of:",
    "options": [
      {
        "text": "k-squares",
        "image": ""
      },
      {
        "text": "k-medians",
        "image": ""
      },
      {
        "text": "k-centers",
        "image": ""
      },
      {
        "text": "k-means",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "The formula ||x-ci||Â² represents squared Euclidean distance. Minimizing the sum of squared distances to cluster centroids is exactly the k-means objective function.",
    "hint": "Think about what the notation 2 means in the subscript."
  },
  {
    "question": "Classification accuracy is misleading when:",
    "options": [
      {
        "text": "the label proportions are unbalanced ",
        "image": ""
      },
      {
        "text": "the label proportions are balanced",
        "image": ""
      },
      {
        "text": "the dataset is too small",
        "image": ""
      },
      {
        "text": "the dataset is too large",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "When classes are imbalanced, a classifier that always predicts the majority class can achieve high accuracy without actually learning meaningful patterns.",
    "hint": "Consider a dataset with 99% negative and 1% positive labels."
  },
  {
    "question": "A binary classifier on 6 points gives the probabilities: 0.85, 0.75, 0.65, 0.5, 0.4, 0.2; the correct labels are 1,1,1,0,0,0. What is the best probability threshold?",
    "options": [
      {
        "text": "0.3",
        "image": ""
      },
      {
        "text": "0.6 ",
        "image": ""
      },
      {
        "text": "0.7",
        "image": ""
      },
      {
        "text": "0.9",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "A threshold of 0.6 perfectly separates all six points: the three with probabilities 0.85, 0.75, 0.65 are above 0.6 (predicted as 1), and the three below 0.6 are predicted as 0, matching all true labels.",
    "hint": "Find the threshold that creates a boundary between the positive and negative probability values."
  },
  {
    "question": "An algorithm is considered practical if its running time, as a function of the input size, is:",
    "options": [
      {
        "text": "exponential",
        "image": ""
      },
      {
        "text": "polynomial ",
        "image": ""
      },
      {
        "text": "linear",
        "image": ""
      },
      {
        "text": "logarithmic",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Polynomial time algorithms (like O(nÂ²), O(nÂ³)) are considered practically tractable, as opposed to exponential time algorithms which become infeasible even for moderate input sizes.",
    "hint": "This is the classic distinction in computational complexity theory."
  },
  {
    "question": "The naive Bayes classier learns:",
    "options": [
      {
        "text": "the marginal distribution of predictors",
        "image": ""
      },
      {
        "text": "the joint distribution of predictors",
        "image": ""
      },
      {
        "text": "the joint distribution of predictors and labels ",
        "image": ""
      },
      {
        "text": "the marginal distribution of labels",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Naive Bayes learns the joint probability distribution P(X,Y) by estimating both P(Y) (prior) and P(X|Y) (likelihood) from training data, then applying Bayes' theorem for classification.",
    "hint": "Recall that Naive Bayes uses Bayes' theorem: P(Y|X) = P(X|Y)P(Y)/P(X)"
  },
  {
    "question": "k-PCA differs from k-means in that xi is:",
    "options": [
      {
        "text": "any PCA component",
        "image": ""
      },
      {
        "text": "any linear combination of PCA components ",
        "image": ""
      },
      {
        "text": "orthogonal to all PCA components",
        "image": ""
      },
      {
        "text": "any a convex combination of PCA components",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In k-PCA, each data point is represented as a linear combination of the kernel principal component directions, which are computed in the feature space induced by the kernel.",
    "hint": "Think about what k-PCA outputs - it's a projection onto new axes (components)."
  },
  {
    "question": "In logistic regression, the estimated probability of xi being a positive is:",
    "options": [
      {
        "text": "1/(1+e-Tx) ",
        "image": ""
      },
      {
        "text": "1/(1+|x|2)",
        "image": ""
      },
      {
        "text": "log(Tx/(1-Tx))",
        "image": ""
      },
      {
        "text": "log(xi)",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Logistic regression uses the sigmoid (logistic) function to transform the linear combination of features into a probability value between 0 and 1.",
    "hint": "Remember the characteristic S-shaped curve in logistic regression."
  },
  {
    "question": "A sports betting agency wants to predict whether the Italian national football team will or not qualify for the World Cup championship. They should use:",
    "options": [
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Logistic regression ",
        "image": ""
      },
      {
        "text": "Linear programming",
        "image": ""
      },
      {
        "text": "Linear regression",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Predicting whether the Italian team will qualify or not is a binary classification problem, and logistic regression is specifically designed for binary classification tasks.",
    "hint": "Consider whether this is a classification or regression problem, and how many possible outcomes exist."
  },
  {
    "question": "A problem X  NP is said to be NP-complete if:",
    "options": [
      {
        "text": "X can be reduced to every Y  NP in polytime",
        "image": ""
      },
      {
        "text": "every Y  NP can be reduced to X in polytime",
        "image": ""
      },
      {
        "text": "no Y  NP can be reduced to X in polytime",
        "image": ""
      },
      {
        "text": "none of the others",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "By definition, a problem X is NP-complete if every problem in NP can be polynomially reduced to X, making X at least as hard as any problem in NP.",
    "hint": "Remember: NP-complete problems are the 'hardest' problems in NP - think about the direction of reduction."
  },
  {
    "question": "The quadratic loss of linear regression is:",
    "options": [
      {
        "text": "i=1m(yi-yi)2",
        "image": ""
      },
      {
        "text": "i=1m(xi-xi)2",
        "image": ""
      },
      {
        "text": "i=1m(xi-yi)2",
        "image": ""
      },
      {
        "text": "i=1m(yi2-yi2)2",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "La perdita quadratica della regressione lineare Ã¨ la somma dei quadrati delle differenze tra i valori osservati (yi) e i valori predetti (Å·i), nota come Mean Squared Error (MSE).",
    "hint": "La regressione lineare minimizza la somma dei quadrati degli errori tra valori reali e predetti."
  },
  {
    "question": "What is the best threshold value for turning probability scores into binary predictions?",
    "options": [
      {
        "text": "the one that maximizes sensitivity",
        "image": ""
      },
      {
        "text": "it depends on the problem ",
        "image": ""
      },
      {
        "text": "the one that maximizes accuracy",
        "image": ""
      },
      {
        "text": "the one that maximizes specificity",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "La soglia ottimale dipende dal problema specifico perchÃ© varia in base al costo relativo dei falsi positivi rispetto ai falsi negativi e all'obiettivo primario del modello.",
    "hint": "In diagnostica medica vs screening generalista, le soglie ideali sono diverse."
  },
  {
    "question": "In Human coding, the encoder:",
    "options": [
      {
        "text": "processes whole runs of identical input symbols",
        "image": ""
      },
      {
        "text": "works by solving a clustering problem",
        "image": ""
      },
      {
        "text": "works by solving a regression problem",
        "image": ""
      },
      {
        "text": "processes each input symbol individually ",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Nella codifica di Huffman (Human coding), ogni simbolo viene processato singolarmente per assegnargli un codice a lunghezza variabile, senza considerare sequenze di simboli identici.",
    "hint": "Distinguere tra codifica run-length e codifica simbolo-per-simbolo."
  },
  {
    "question": "The Maximum Likelihood Estimator for the parameters of a linear model with independent Gaussian noise is:",
    "options": [
      {
        "text": "the OLS solution vector *  ",
        "image": ""
      },
      {
        "text": "the square root of the OLS solution *",
        "image": ""
      },
      {
        "text": "it depends on the dataset",
        "image": ""
      },
      {
        "text": "the vector  of the generating process",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Sotto l'assunzione di rumore Gaussiano indipendente con varianza costante, lo stimatore di massima verosimiglianza (MLE) coincide esattamente con la soluzione dei minimi quadrati ordinari (OLS).",
    "hint": "Il rumore Gaussiano con varianza costante porta a MLE = OLS."
  },
  {
    "question": "Consider the LP: min f(x,y)=x+y; x+y2; x,y0. The corresponding polytope is:",
    "options": [
      {
        "text": "degenerate",
        "image": ""
      },
      {
        "text": "bounded",
        "image": ""
      },
      {
        "text": "unbounded",
        "image": ""
      },
      {
        "text": "empty ",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Il vincolo x+yâ‰¤2 (se interpretato come â‰¤) con x,yâ‰¥0 definisce un'area limitata (triangolo), ma se i vincoli sono inconsistenti tra loro (come x+yâ‰¤2 e x+yâ‰¥2 che si escludono a vicenda), il politopo Ã¨ vuoto.",
    "hint": "Un politopo Ã¨ vuoto quando non esiste nessun punto che soddisfi tutti i vincoli contemporaneamente."
  },
  {
    "question": "Min-hashing maps each document to:",
    "options": [
      {
        "text": "one hash signature ",
        "image": ""
      },
      {
        "text": "a distance matrix",
        "image": ""
      },
      {
        "text": "the set of most frequent terms",
        "image": ""
      },
      {
        "text": "a real vector",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Min-hashing is a technique for estimating Jaccard similarity between sets. It converts each document (represented as a set of elements) into a compact signature (a set of hash values) that can be used to quickly estimate similarity between documents.",
    "hint": "Think about what 'signature' means in the context of locality-sensitive hashing."
  },
  {
    "question": "How do you do a linear regression in R?",
    "options": [
      {
        "text": "predict(y  x, data)",
        "image": ""
      },
      {
        "text": "lm(y  x, data) ",
        "image": ""
      },
      {
        "text": "predict(y  x, data, family=\"binomial\")",
        "image": ""
      },
      {
        "text": "lm(y  x, data, family=\"binomial\")",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "In R, the lm() function is the standard way to fit linear models. The formula notation y ~ x specifies the relationship between the dependent variable (y) and the independent variable (x), with the data parameter specifying the data frame.",
    "hint": "Remember that 'lm' stands for linear model and uses formula syntax in R."
  },
  {
    "question": "How do you measure the significance of an estimate?",
    "options": [
      {
        "text": "with its magnitude",
        "image": ""
      },
      {
        "text": "with R^2",
        "image": ""
      },
      {
        "text": "with its p-value",
        "image": ""
      },
      {
        "text": "with its sign",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The p-value measures the probability of observing the estimated effect (or more extreme) if the null hypothesis were true. It indicates whether an estimate is statistically significant, helping determine if an observed relationship is likely real or due to chance.",
    "hint": "Think about hypothesis testing and what statistical measure tells you if an effect is likely not due to random chance."
  },
  {
    "question": "A manufacturing company wants to nd out the relationship between the budget spent in advertising and the total sales of the next semester. They could use:",
    "options": [
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Linear regression is used when the target variable is continuous. Here, both advertising budget (predictor) and total sales (outcome) are continuous variables, and we want to understand their linear relationship.",
    "hint": "Consider the type of the target variable - sales is a continuous quantity, not a category."
  },
  {
    "question": "The company wants to predict if a machine will have a technical failure in the next 10 days. This could be done with:",
    "options": [
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Logistic regression is a classification method used for binary outcomes. Predicting whether a machine will fail (yes/no) is a binary classification problem, making logistic regression the appropriate choice.",
    "hint": "The outcome is binary - the machine either will fail or will not fail in the next 10 days."
  },
  {
    "question": "Moreover, items from the same production line are similar while those from different lines are radically different. You suggest to check by using:",
    "options": [
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Il clustering Ã¨ la tecnica di apprendimento non supervisionato che raggruppa automaticamente punti simili nello stesso cluster e diversi in cluster separati, senza bisogno di etichetteé¢„å…ˆ.",
    "hint": "Cerca una tecnica che trovi gruppi naturali nei dati senza usare variabili obiettivo."
  },
  {
    "question": "What is the true positive rate aka sensitivity?",
    "options": [
      {
        "text": "the fraction of negatives that are incorrectly classified",
        "image": ""
      },
      {
        "text": "the fraction of negatives that are correctly classified",
        "image": ""
      },
      {
        "text": "the fraction of positives that are incorrectly classified",
        "image": ""
      },
      {
        "text": "the fraction of positives that are correctly classified",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "La sensitivity o true positive rate misura la capacitÃ  del modello di identificare correttamente tutti i casi positivi reali, ovvero la proporzione di positivi che il modello ha classificato come positivi.",
    "hint": "Ricorda la differenza tra vera positivitÃ  (correctly classified positives) e falsa positivitÃ ."
  },
  {
    "question": "Single-linkage clustering works by:",
    "options": [
      {
        "text": "repeatedly recomputing the centroids of clusters",
        "image": ""
      },
      {
        "text": "repeatedly merging smaller clusters into larger ones",
        "image": ""
      },
      {
        "text": "enumerating all possible clustering of the given points",
        "image": ""
      },
      {
        "text": "enumerating all possible points in a cluster",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Single-linkage Ã¨ un algoritmo di clustering gerarchico agglomerativo che parte da cluster individuali e ripetibilmente unisce i due cluster con la distanza minima tra i loro punti piÃ¹ vicini.",
    "hint": "Ãˆ un metodo gerarchico che parte dai singoli punti e unisce progressivamente i cluster piÃ¹ vicini."
  },
  {
    "question": "You have a set of observations (x; y) with x; y 2 R. Which one of the following gives the highest R2?",
    "options": [
      {
        "text": "Simple linear regression",
        "image": ""
      },
      {
        "text": "Polynomial regression of degree 2",
        "image": ""
      },
      {
        "text": "Polynomial regression of degree 10",
        "image": ""
      },
      {
        "text": "Logistic regression\t",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Un polinomio di grado maggiore ha maggiore capacitÃ  di flessione e puÃ² sempre adattarsi perfettamente o meglio ai dati di training, aumentando quindi RÂ² rispetto a modelli piÃ¹ semplici.",
    "hint": "Modelli con piÃ¹ parametri possono sempre replicare (o superare) le prestazioni di modelli piÃ¹ semplici sui stessi dati."
  },
  {
    "question": "Which one of the following performances indicates the best model for prediction?",
    "options": [
      {
        "text": "R2 = 0:2 on training, R2 = 0:1 on test",
        "image": ""
      },
      {
        "text": "R2 = 0:7 on training, R2 = 0:7 on test",
        "image": ""
      },
      {
        "text": "R2 = 0:8 on training, R2 = 0:1 on test",
        "image": ""
      },
      {
        "text": "R2 = 0:9 on training, R2 = ô€€€0:9 on test",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Un buon modello predittivo deve generalizzare, ovvero avere prestazioni simili su dati di training e test. L'opzione B mostra prestazioni buone e consistenti su entrambi i set, indicando assenza di overfitting.",
    "hint": "Cerca la combinazione con la minor differenza tra training e test, oltre a prestazioni accettabili."
  },
  {
    "question": "Which task does not require a training set (i.e. a dataset used for learning a model)?",
    "options": [
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Classification",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Clustering is an unsupervised learning technique that groups data based on similarity without requiring labeled examples - it discovers structure in data without learning from a training set with known outcomes.",
    "hint": "Think about whether the method needs labeled examples to learn."
  },
  {
    "question": "If you have n points, what is the number of clusters that minimizes the within-cluster sum of squares?",
    "options": [
      {
        "text": "1",
        "image": ""
      },
      {
        "text": "k",
        "image": ""
      },
      {
        "text": "n",
        "image": ""
      },
      {
        "text": "we cannot say",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "When each point forms its own cluster (n clusters), the within-cluster sum of squares is exactly zero since there's no variance within a single point - this is the mathematical minimum, though not useful in practice.",
    "hint": "Consider what happens when each point becomes its own cluster."
  },
  {
    "question": "In the bias-variance decomposition of the expected squared error, what does a high bias suggest?",
    "options": [
      {
        "text": "noisy data",
        "image": ""
      },
      {
        "text": "overtting",
        "image": ""
      },
      {
        "text": "undertting",
        "image": ""
      },
      {
        "text": "crosstting",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "High bias means the model makes overly simplistic assumptions and fails to capture the underlying data pattern, which is the definition of underfitting.",
    "hint": "Think about whether the model is too simple to learn the data structure."
  },
  {
    "question": "A set of observations (x1; y1), (x2; y2), â€¦,(xn; yn) obeys the law yi := axi + b + i where  i is some random noise. The task of estimating a and b from the dataset is called:",
    "options": [
      {
        "text": "logistic regression",
        "image": ""
      },
      {
        "text": "linear regression",
        "image": ""
      },
      {
        "text": "linear programming",
        "image": ""
      },
      {
        "text": "logistic programming",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "This describes the classic linear regression problem: estimating the linear relationship y = ax + b + Îµ where a and b are the slope and intercept parameters to be learned from data with noise.",
    "hint": "What method estimates linear relationships between variables with random noise?"
  },
  {
    "question": "A regression model (M1) on a training set gives R^2 = 0.5 while a second model (M2) gives R^2 = 0.9. What can we say about predictions on a test set?",
    "options": [
      {
        "text": "M2 will have error smaller than M1",
        "image": ""
      },
      {
        "text": "M2 will have error larger than M1",
        "image": ""
      },
      {
        "text": "M2 will have the same error as M1",
        "image": ""
      },
      {
        "text": "we cannot say",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "RÂ² measures fit on training data only. A higher training RÂ² doesn't guarantee better test performance due to potential overfitting, and test error depends on many factors beyond training RÂ².",
    "hint": "Remember that training performance doesn't always translate to test performance."
  },
  {
    "question": "You developed a clinical test to distinguish sick patients from healthy patients. In the population, on average 998 out of 1000 people are healthy, and the test gives an incorrect prediction in 0.5% of the cases. This means the test:",
    "options": [
      {
        "text": "identifies all the healthy patients",
        "image": ""
      },
      {
        "text": "identifies all the sick patients",
        "image": ""
      },
      {
        "text": "could miss all the healthy patients",
        "image": ""
      },
      {
        "text": "could miss all the sick patients",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "With only 2 sick patients out of 1000 and a 0.5% error rate, it's statistically possible (though unlikely) that both sick patients receive false negatives, meaning the test could miss all sick patients.",
    "hint": "Consider the small number of sick patients and what the 0.5% error means for each group."
  },
  {
    "question": "How would you describe overfitting?",
    "options": [
      {
        "text": "the model is too complex and follows the noise",
        "image": ""
      },
      {
        "text": "the model is too complex and discards the noise",
        "image": ""
      },
      {
        "text": "the model is too simple and follows the noise",
        "image": ""
      },
      {
        "text": "the model is too simple and discards the noise",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Overfitting occurs when a model is so complex it learns the random noise in training data rather than the true underlying pattern, failing to generalize to new data.",
    "hint": "Think about what 'following the noise' means for model performance on new data."
  },
  {
    "question": "You have to convert the scores given by a logistic regression model into binary predictions. What is the best threshold?",
    "options": [
      {
        "text": "the one that maximizes accuracy",
        "image": ""
      },
      {
        "text": "the one that maximizes TPR",
        "image": ""
      },
      {
        "text": "the one that maximizes FPR",
        "image": ""
      },
      {
        "text": "it depends on the requirements",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "The optimal threshold depends on the contextâ€”for some applications minimizing false negatives is critical, for others minimizing false positives matters more, so there's no universal best threshold.",
    "hint": "Consider whether a false positive or false negative is more costly in different scenarios."
  },
  {
    "question": "Given a linear regression model, the expected squared error can be usefully decomposed in:",
    "options": [
      {
        "text": "SSE and SST",
        "image": ""
      },
      {
        "text": "underfit, overfit and noise",
        "image": ""
      },
      {
        "text": "bias, variance, and error",
        "image": ""
      },
      {
        "text": "variance and covariance",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "The expected squared error in linear regression decomposes into bias (systematic error from model assumptions), variance (sensitivity to training data), and irreducible error (noise).",
    "hint": "This is the fundamental bias-variance tradeoff decomposition."
  },
  {
    "question": "Look at the confusion matrix below (1=positive=true,0=negative=false). What can we say?",
    "options": [
      {
        "text": "the specificity is 2/3",
        "image": ""
      },
      {
        "text": "the sensitivity is 2/3",
        "image": ""
      },
      {
        "text": "the accuracy is 2/3",
        "image": ""
      },
      {
        "text": "none of the above",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "https://raw.githubusercontent.com/dag7dev/UniQuizzes/master/img/FDS/1positive0negative.png.png",
    "code": "",
    "explanation": "Specificity = TN/(TN+FP). From the confusion matrix, TN=2, FP=1, so specificity = 2/(2+1) = 2/3.",
    "hint": "Remember specificity is calculated from the negative class row."
  },
  {
    "question": "You are using k-means, and notice that different executions give different results. This happens since:",
    "options": [
      {
        "text": "k-means is randomized",
        "image": ""
      },
      {
        "text": "clustering can take exponential time",
        "image": ""
      },
      {
        "text": "this is unsupervised learning",
        "image": ""
      },
      {
        "text": "you are using the wrong value for k",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "k-means utilizza un'inizializzazione random dei centroidi (k-means++) e puÃ² convergere a minimi locali diversi ad ogni esecuzione, producendo risultati diversi.",
    "hint": "Il problema non Ã¨ il tempo nÃ© il fatto che sia non supervisionato, ma l'inizializzazione randomica."
  },
  {
    "question": "You have 6 observations; their class (Positive or Negative) and the score given by a logistic regression are as follows: (P,0.9), (P,0.85), (N,0.75), (P,0.5), (N,0.4), (N,0.3). If you do not want the false positive rate of your classier to exceed 1/3, the best choice is to predict â€œY\" whenever the score is at least:",
    "options": [
      {
        "text": "1.2",
        "image": ""
      },
      {
        "text": "1.0",
        "image": ""
      },
      {
        "text": "0.45",
        "image": ""
      },
      {
        "text": "0.25",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Per avere FPR = FP/(FP+TN) â‰¤ 1/3 con 3 negativi, servono al massimo 1 FP. Con soglia 0.45, solo il negativo con score 0.75 viene classificato come Y, dando FP=1 e FPR=1/3.",
    "hint": "Calcola il FPR per ogni possibile soglia e verifica quale soddisfa il vincolo."
  },
  {
    "question": "Logistic regression finds the parameters that maximize: ",
    "options": [
      {
        "text": "the mean square error of the input data",
        "image": ""
      },
      {
        "text": "the skewness of the input data",
        "image": ""
      },
      {
        "text": "the inter-cluster distance of the input data",
        "image": ""
      },
      {
        "text": "the log-likelihood of the input data",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "La regressione logistica stima i parametri massimizzando la log-verosimiglianza dei dati, equivalentemente a minimizzare la log-loss (cross-entropy).",
    "hint": "Ãˆ un metodo di massima verosimiglianza, non di errore quadratico medio."
  },
  {
    "question": "What does the Bayesian Optimal Classier need to know in order to work?",
    "options": [
      {
        "text": "the marginal distribution of each variable",
        "image": ""
      },
      {
        "text": "the marginal distribution of the label",
        "image": ""
      },
      {
        "text": "the joint distribution of variables and label",
        "image": ""
      },
      {
        "text": "the joint distribution of the variables",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Il classificatore bayesiano ottimale sceglie la classe con massima P(y|x) = P(x,y)/P(x), quindi richiede la distribuzione congiunta completa di variabili e label.",
    "hint": "Per calcolare P(y|x) serve la distribuzione condizionale, che deriva dalla distribuzione congiunta."
  },
  {
    "question": "Which one of the following classifiers has the best performance?",
    "options": [
      {
        "text": "TPR=0.2, FPR=0.2",
        "image": ""
      },
      {
        "text": "TPR=0.2, FPR=0.8",
        "image": ""
      },
      {
        "text": "TPR=0.8, FPR=0.2",
        "image": ""
      },
      {
        "text": "TPR=0.8, FPR=0.8",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Un buon classificatore deve avere TPR alto (alta sensibilitÃ ) e FPR basso (alta specificitÃ ). L'opzione C con TPR=0.8 e FPR=0.2 domina le altre (maggiore TPR e minore FPR).",
    "hint": "Confronta le coppie: devi massimizzare TPR e minimizzare FPR simultaneamente."
  },
  {
    "question": "Your boss calls you to tell your new regression model seems completely useless for prediction, in spite of the high R^2 of the t. You realize that probably there is:",
    "options": [
      {
        "text": "underfitting",
        "image": ""
      },
      {
        "text": "overfitting",
        "image": ""
      },
      {
        "text": "correlation",
        "image": ""
      },
      {
        "text": "no tomorrow",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "L'overfitting si verifica quando il modello memorizza il rumore nei dati di training invece di apprendere pattern generalizzabili. Un alto R^2 sui dati di training puÃ² indicare che il modello si Ã¨ adattato troppo bene ai dati specifici, perdendo capacitÃ  predittiva su nuovi dati.",
    "hint": "Pensa a cosa succede quando un modello 'impara a memoria' invece di 'comprendere'."
  },
  {
    "question": "From the confusion matrix below, what can we say?",
    "options": [
      {
        "text": "R^2 = 0:67",
        "image": ""
      },
      {
        "text": "accuracy = 80%",
        "image": ""
      },
      {
        "text": "all good things must come to an end",
        "image": ""
      },
      {
        "text": "sensitivity < 80%",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "https://raw.githubusercontent.com/dag7dev/UniQuizzes/master/img/FDS/accuracy80.png.png",
    "code": "",
    "explanation": "L'accuracy Ã¨ definita come (veri positivi + veri negativi) / totale osservazioni. Se dalla matrice si ricava l'80% di classificazioni corrette, allora l'accuracy Ã¨ 80%.",
    "hint": "Ricorda la formula base dell'accuracy: (TP + TN) / total."
  },
  {
    "question": "Consider the LP: max f(x,y)=x+3y; x10; y3. The value of the optimal solution is:",
    "options": [
      {
        "text": "19",
        "image": ""
      },
      {
        "text": "23",
        "image": ""
      },
      {
        "text": "12",
        "image": ""
      },
      {
        "text": "40",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Per massimizzare f(x,y) = x + 3y con i vincoli x â‰¤ 10 e y â‰¤ 3, si scelgono i valori massimi ammissibili: x=10, y=3. Il valore ottimale Ã¨ 10 + 3Ã—3 = 19.",
    "hint": "Nella programmazione lineare, la soluzione ottimale si trova sempre ai vertici della regione fattibile."
  },
  {
    "question": "Your friend proposes a novel clustering algorithm that tries all possible clusterings of the data. This algorithm:",
    "options": [
      {
        "text": "has exponential complexity",
        "image": ""
      },
      {
        "text": "is efficient but gives poor clusterings",
        "image": ""
      },
      {
        "text": "has polynomial complexity",
        "image": ""
      },
      {
        "text": "is efficient and gives good clusterings",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Il numero di possibili partizioni (clustering) di n punti dati cresce esponenzialmente (numeri di Bell). Provarle tutte richiede quindi tempo esponenziale, rendendo l'algoritmo computazionalmente infattibile per dataset di dimensioni realistiche.",
    "hint": "Il numero di modi per dividere n elementi in gruppi cresce molto rapidamente."
  },
  {
    "question": "In a binary classier build by thresholding the scores of a logistic regression model, the positive observations:",
    "options": [
      {
        "text": "have a score strictly higher than all the negatives",
        "image": ""
      },
      {
        "text": "have higher density than the negatives",
        "image": ""
      },
      {
        "text": "are at least as many as the negatives",
        "image": ""
      },
      {
        "text": "are separated from the negatives by a hyperplane",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "La regressione logistica stima un modello lineare che produce un confine di decisione (iperpiano) che separa le osservazioni positive da quelle negative in base al punteggio (log-odds) prodotto dal modello.",
    "hint": "La logistic regression trova un confine di separazione lineare tra le classi."
  },
  {
    "question": "The class NP contains all problems whose solution:",
    "options": [
      {
        "text": "can be verified in polytime",
        "image": ""
      },
      {
        "text": "requires exponential time",
        "image": ""
      },
      {
        "text": "none of the others",
        "image": ""
      },
      {
        "text": "can be computed in polytime",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "NP (Nondeterministic Polynomial) contains problems whose solutions can be verified in polynomial time given a certificate, even if finding the solution may require exponential time.",
    "hint": "P in NP stands for verification, not computation."
  },
  {
    "question": "Lloyd's algorithm for k-means works by:",
    "options": [
      {
        "text": "evaluating all possible points in a cluster",
        "image": ""
      },
      {
        "text": "evaluating all possible clustering of the points",
        "image": ""
      },
      {
        "text": "repeatedly merging clusters",
        "image": ""
      },
      {
        "text": "repeatedly adjusting the centroids of clusters",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Lloyd's algorithm iteratively alternates between assigning points to the nearest centroid and recomputing each centroid as the mean of its assigned points until convergence.",
    "hint": "The algorithm has two main steps: assignment and centroid update."
  },
  {
    "question": "You want to learn how your revenue depends on parameters such as number of working hours, etc. You could use:",
    "options": [
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Linear regression is used to model the relationship between a continuous dependent variable (revenue) and independent variables (working hours, etc.).",
    "hint": "You're predicting a continuous quantity (revenue) from other variables."
  },
  {
    "question": "In least squares, R^2 can be seen as:",
    "options": [
      {
        "text": "the norm of the parameter vector",
        "image": ""
      },
      {
        "text": "none of the others",
        "image": ""
      },
      {
        "text": "the gain over a baseline model",
        "image": ""
      },
      {
        "text": "the inverse of the SSE",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "RÂ² represents the proportion of variance explained by the model compared to a baseline model that always predicts the mean; it's the gain over simply predicting the average.",
    "hint": "Compare the model's performance to a trivial baseline."
  },
  {
    "question": "The ROC curve shows:",
    "options": [
      {
        "text": "specificity versus sensitivity",
        "image": ""
      },
      {
        "text": "specificity versus FPR",
        "image": ""
      },
      {
        "text": "TPR versus sensitivity",
        "image": ""
      },
      {
        "text": "TPR versus FPR",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "The ROC curve plots True Positive Rate (sensitivity/recall) on the y-axis against False Positive Rate (1-specificity) on the x-axis at various classification thresholds.",
    "hint": "It shows how TPR changes as we vary the decision threshold."
  },
  {
    "question": "Can feature scaling improve the model fitted via least squares?",
    "options": [
      {
        "text": "yes, in terms of p-values",
        "image": ""
      },
      {
        "text": "no",
        "image": ""
      },
      {
        "text": "yes, in terms of interpretability",
        "image": ""
      },
      {
        "text": "yes, in terms of R2",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Feature scaling does not change the fitted values, RÂ², or p-values in least squares regression because the estimates are scale-invariant. However, scaling makes coefficients comparable and interpretable, especially when predictors have different units.",
    "hint": "Recall that least squares coefficients don't depend on the scale of predictors, but interpretation can change."
  },
  {
    "question": "Can a clustering on n points achieve 0 within-cluster sum of squares?",
    "options": [
      {
        "text": "yes, with 1 cluster",
        "image": ""
      },
      {
        "text": "yes, with k clusters",
        "image": ""
      },
      {
        "text": "yes, with n clusters",
        "image": ""
      },
      {
        "text": "no, never",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "When each point forms its own cluster (n clusters), the within-cluster variance is zero because there's only one point per cluster and no dispersion within a single observation.",
    "hint": "Think about what happens to variance when there's only one point in a group."
  },
  {
    "question": "In linear regression, if the p-value for the estimate i is small enough, then we:",
    "options": [
      {
        "text": "accept the null hypothesis i = 0",
        "image": ""
      },
      {
        "text": "reject the null hypothesis i = 0",
        "image": ""
      },
      {
        "text": "use a model with more features",
        "image": ""
      },
      {
        "text": "use a model with more parameters",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "A small p-value provides strong evidence to reject the null hypothesis that the coefficient equals zero, indicating the predictor is statistically significant.",
    "hint": "Small p-values mean we reject the null hypothesis that there's no effect."
  },
  {
    "question": "Texts written in the same language have a similar letter frequency distribution. You can check this fact by:",
    "options": [
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      },
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "This is an unsupervised learning problem where we want to group documents by similarity without predefined labels. Clustering algorithms can identify natural groupings based on letter frequency distributions.",
    "hint": "Consider whether we have labeled data or are trying to discover groups automatically."
  },
  {
    "question": "Texts written in the same language have a similar letter frequency distribution. You can check this fact by:",
    "options": [
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      },
      {
        "text": "Linear Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "This is an unsupervised learning problem where we want to group documents by similarity without predefined labels. Clustering algorithms can identify natural groupings based on letter frequency distributions.",
    "hint": "Consider whether we have labeled data or are trying to discover groups automatically."
  },
  {
    "question": "Two classifiers, C1 and C2, have accuracy respectively 98\\\\% \\\\and 95%. Which one is the best?",
    "options": [
      {
        "text": "C1",
        "image": ""
      },
      {
        "text": "They are equivalent",
        "image": ""
      },
      {
        "text": "We cannot say",
        "image": ""
      },
      {
        "text": "C2",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Accuracy alone is insufficient to determine the best classifier. We need additional information such as dataset size, class distribution, statistical significance of the difference, and the specific application context to make a proper comparison.",
    "hint": "What other factors beyond accuracy percentages should we consider when comparing classifiers?"
  },
  {
    "question": "Correlation clustering asks to minimize:",
    "options": [
      {
        "text": "The root mean squared error",
        "image": ""
      },
      {
        "text": "The number of disagreements",
        "image": ""
      },
      {
        "text": "The intra-cluster variance",
        "image": ""
      },
      {
        "text": "The running time",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Correlation clustering minimizes the number of disagreements, which includes both co-clustering of dissimilar pairs and separation of similar pairs. This is a measure of how well the clustering matches the pairwise similarity relationships.",
    "hint": "Think about what 'disagreement' means when comparing pairs of data points."
  },
  {
    "question": "If you increase the complexity of your linear regression model, eventually the SSE on the test set will:",
    "options": [
      {
        "text": "Approach zero",
        "image": ""
      },
      {
        "text": "Cancel the training error",
        "image": ""
      },
      {
        "text": "Exceed the training error",
        "image": ""
      },
      {
        "text": "Become negative",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "As model complexity increases beyond the optimal point, the model begins to overfit by capturing noise in the training data. While training error continues to decrease, test error increases because the model fails to generalize to unseen data.",
    "hint": "What happens to generalization when complexity grows too large?"
  },
  {
    "question": "Classification accuracy is misleading when:",
    "options": [
      {
        "text": "The label proportions are unbalanced",
        "image": ""
      },
      {
        "text": "The dataset is too small",
        "image": ""
      },
      {
        "text": "The label proprtions are balanced",
        "image": ""
      },
      {
        "text": "The dataset is too large",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "With unbalanced label proportions, a trivial classifier predicting only the majority class can achieve high accuracy without learning meaningful patterns. For instance, in a 99%-1% class distribution, always predicting the majority class yields 99% accuracy.",
    "hint": "Consider a dataset where one class dominates - what accuracy would a naive classifier achieve?"
  },
  {
    "question": "The worst-case running time of the k-means algorithm on the n points is:",
    "options": [
      {
        "text": "Polynomial in n",
        "image": ""
      },
      {
        "text": "Superpolynomial in n",
        "image": ""
      },
      {
        "text": "Linear in n",
        "image": ""
      },
      {
        "text": "Unbounded in n",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "K-means does not have a polynomial-time guarantee in the worst case. Although it usually converges quickly in practice, theoretically it can take exponential time as it may cycle through different cluster assignments without converging.",
    "hint": "Does k-means have theoretical guarantees on convergence time?"
  },
  {
    "question": "In linear regression, the expected squared error s the sum of:",
    "options": [
      {
        "text": "The good the bad and the ugly",
        "image": ""
      },
      {
        "text": "Squared bias and variance and noise",
        "image": ""
      },
      {
        "text": "Underfit and overfit the noise",
        "image": ""
      },
      {
        "text": "Variance and covariance and noise",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "L'errore quadratico atteso nella regressione lineare si decompone in tre termini: il bias al quadrato (squared bias), la varianza e l'errore irriducibile (rumore). Questo Ã¨ il celebre trade-off bias-varianza della teoria statistica dell'apprendimento.",
    "hint": "Ricorda la classica decomposizione dell'errore di generalizzazione in statistica."
  },
  {
    "question": "Your friend proposes an innovative clustering algorithm that enumerates all possible clusterings of the points. This algorithm:",
    "options": [
      {
        "text": "Has exponential complexity",
        "image": ""
      },
      {
        "text": "Has polynomial complexity",
        "image": ""
      },
      {
        "text": "Is efficient but gives poor clustering",
        "image": ""
      },
      {
        "text": "Is efficient and gives good clusterings",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Il numero di possibili partizioni di n punti (numeri di Bell) cresce esponenzialmente con n, rendendo computazionalmente impossibile enumerare tutte le possibili configurazioni di clustering per dataset di dimensioni realistiche.",
    "hint": "Pensa a quanto rapidamente grows il numero di modi per dividere n elementi in gruppi."
  },
  {
    "question": "A high R^2 on a given dataset means:",
    "options": [
      {
        "text": "A large error on new data",
        "image": ""
      },
      {
        "text": "A large error on that data",
        "image": ""
      },
      {
        "text": "A small error on that data",
        "image": ""
      },
      {
        "text": "A small error on new data",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "RÂ² misura la proporzione di varianza della variabile dipendente spiegata dal modello sui dati di training. Un valore alto indica semplicemente un buon fit sui dati su cui il modello Ã¨ stato addestrato, non garantendo prestazioni su dati nuovi.",
    "hint": "RÂ² Ã¨ una metrica in-sample, non misura la capacitÃ  di generalizzazione."
  },
  {
    "question": "Multicollinearity arises if the features vectors are:",
    "options": [
      {
        "text": "absolutely orthonogal",
        "image": ""
      },
      {
        "text": "linearly dependent",
        "image": ""
      },
      {
        "text": "linearly independent",
        "image": ""
      },
      {
        "text": "positive semidefinite",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "La multicollinearitÃ  si verifica quando almeno una feature puÃ² essere espressa come combinazione lineare delle altre, rendendo la matrice delle feature (o la matrice di design) non invertibile o mal condizionata.",
    "hint": "Una matrice con vettori colonne linearmente dipendenti Ã¨ singolare."
  },
  {
    "question": "A logistic regression model learns:",
    "options": [
      {
        "text": "The conditional distribution of predictors",
        "image": ""
      },
      {
        "text": "The conditional distribution of labels",
        "image": ""
      },
      {
        "text": "The marginal distribution of predictors",
        "image": ""
      },
      {
        "text": "The marginal distribution of labels",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "La regressione logistica stima la distribuzione condizionata P(Y|X), cioÃ¨ la probabilitÃ  delle etichette (variabile dipendente) dato i predittori, attraverso la funzione sigmoide.",
    "hint": "La logistic regression modella la probabilitÃ  di appartenenza a una classe condizionata ai feature."
  },
  {
    "question": "Consider the LP: min f(x,y) = x + y; x+y >= 2; x, y <= 0. The corresponding polytope is:",
    "options": [
      {
        "text": "Bounded",
        "image": ""
      },
      {
        "text": "empty",
        "image": ""
      },
      {
        "text": "Degenerate",
        "image": ""
      },
      {
        "text": "Unbounded",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "The constraints x â‰¤ 0 and y â‰¤ 0 imply x + y â‰¤ 0, but the constraint x + y â‰¥ 2 requires the sum to be at least 2. These contradictory requirements make the feasible region empty.",
    "hint": "Check if all constraints can be satisfied simultaneously."
  },
  {
    "question": "To measure the efficiency of algorithms we use:",
    "options": [
      {
        "text": "convex analysis",
        "image": ""
      },
      {
        "text": "asymptotic analysis",
        "image": ""
      },
      {
        "text": "squared analysis",
        "image": ""
      },
      {
        "text": "clinical analysis",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Asymptotic analysis (Big-O, Î©, Î˜ notation) is the standard method for measuring algorithm efficiency, describing how runtime scales with input size.",
    "hint": "Think about how we describe algorithm complexity in computer science."
  },
  {
    "question": "Everything else being equal, what does suggest a good clustering?",
    "options": [
      {
        "text": "a high p-value",
        "image": ""
      },
      {
        "text": "a low within-cluster sum of squares",
        "image": ""
      },
      {
        "text": "a large number of observations",
        "image": ""
      },
      {
        "text": "a small number of clusters",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "A low within-cluster sum of squares (WCSS) indicates that data points are close to their cluster centroids, meaning clusters are compact and well-defined.",
    "hint": "Consider what makes points in the same cluster similar to each other."
  },
  {
    "question": "The set cover problem:",
    "options": [
      {
        "text": "Can be solved in constant time",
        "image": ""
      },
      {
        "text": "is part of linear programming",
        "image": ""
      },
      {
        "text": "is NP-Complete",
        "image": ""
      },
      {
        "text": "is P-Complete",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Set cover is a classic NP-Complete problem, meaning there is no known polynomial-time algorithm to solve it exactly.",
    "hint": "Recall the famous NP-complete problems from computational complexity theory."
  },
  {
    "question": "A company must allocate 5Mâ‚¬ so that each department receives a minimum amount. You can use:",
    "options": [
      {
        "text": "Linear regression",
        "image": ""
      },
      {
        "text": "Logistic Regression",
        "image": ""
      },
      {
        "text": "Clustering",
        "image": ""
      },
      {
        "text": "Linear Programming",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "Clustering is used to group entities (departments) into homogeneous segments based on shared characteristics, which is appropriate for dividing a budget among groups.",
    "hint": "Think about what technique groups similar entities together."
  },
  {
    "question": "With hierarchical clustering on n points you can get:",
    "options": [
      {
        "text": "Between 1 and n clusters",
        "image": ""
      },
      {
        "text": "No satisfaction",
        "image": ""
      },
      {
        "text": "Up to 2^n clusters",
        "image": ""
      },
      {
        "text": "At most log(n) clusters",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "Hierarchical clustering produces a dendrogram where each merge reduces the number of clusters by one, starting from n single-point clusters down to 1 cluster containing all points.",
    "hint": "Think about the two extremes: when should the clustering stop?"
  },
  {
    "question": "The standard assumption of linear regression is that the noise across the observations:",
    "options": [
      {
        "text": "is fast and furios",
        "image": ""
      },
      {
        "text": "is always bounded",
        "image": ""
      },
      {
        "text": "is Gaussian and correlated",
        "image": ""
      },
      {
        "text": "is Gaussian and independent",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "The Gauss-Markov theorem assumes normally distributed, uncorrelated errors for OLS to be the best linear unbiased estimator. Independence ensures each observation provides unique information.",
    "hint": "What are the standard assumptions for ordinary least squares?"
  },
  {
    "question": "The ROC curve is used to measure:",
    "options": [
      {
        "text": "The amount of overfitting and underfitting",
        "image": ""
      },
      {
        "text": "The noise in the training dataset",
        "image": ""
      },
      {
        "text": "The performance of binary classifiers",
        "image": ""
      },
      {
        "text": "The MSE obtained by a linear regression\"",
        "image": ""
      }
    ],
    "correctIndex": 2,
    "image": "",
    "code": "",
    "explanation": "ROC (Receiver Operating Characteristic) curves plot the true positive rate against the false positive rate at various classification thresholds, evaluating how well a binary classifier discriminates between classes.",
    "hint": "What does the curve show when you vary the classification threshold?"
  },
  {
    "question": "Geometrically, each constraint of a linear program corresponds to:",
    "options": [
      {
        "text": "a vector",
        "image": ""
      },
      {
        "text": "A double-space",
        "image": ""
      },
      {
        "text": "a cone",
        "image": ""
      },
      {
        "text": "a half-space",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Each linear constraint defines a half-space (the region on one side of a hyperplane). The feasible region is the intersection of all these half-spaces, forming a convex polytope.",
    "hint": "What geometric region does a linear inequality define?"
  },
  {
    "question": "Many well-known clustering problems are:",
    "options": [
      {
        "text": "impossible to solve",
        "image": ""
      },
      {
        "text": "NP-hard",
        "image": ""
      },
      {
        "text": "easy to solve",
        "image": ""
      },
      {
        "text": "infeasible\"",
        "image": ""
      }
    ],
    "correctIndex": 1,
    "image": "",
    "code": "",
    "explanation": "Many clustering problems like k-means and hierarchical clustering with certain linkage criteria are NP-hard, meaning no polynomial-time algorithm is known for finding exact solutions in general.",
    "hint": "Think about the combinatorial nature of partitioning n points into k groups."
  },
  {
    "question": "A polytope is:",
    "options": [
      {
        "text": "The difference of half-spaces",
        "image": ""
      },
      {
        "text": "the greatest gift of all",
        "image": ""
      },
      {
        "text": "the union of half-spaces",
        "image": ""
      },
      {
        "text": "The intersection of half spaces",
        "image": ""
      }
    ],
    "correctIndex": 3,
    "image": "",
    "code": "",
    "explanation": "Un politopo Ã¨ definito matematicamente come l'insieme di tutti i punti che soddisfano un sistema finito di disequazioni lineari, cioÃ¨ l'intersezione di semispazi chiusi. Questa Ã¨ la definizione fondamentale nella teoria dei poliedri e nella geometria computazionale.",
    "hint": "Ricorda la definizione formale: un politopo Ã¨ l'insiemeé™ definito da disequazioni lineari."
  },
  {
    "question": "Everything else being equal. What does suggest good clustering?",
    "options": [
      {
        "text": "Few clusters",
        "image": ""
      },
      {
        "text": "low within-cluster sum of squares",
        "image": ""
      },
      {
        "text": "high p-value",
        "image": ""
      },
      {
        "text": "large number of points",
        "image": ""
      }
    ],
    "correctIndex": 0,
    "image": "",
    "code": "",
    "explanation": "",
    "hint": ""
  }
]